{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-insarscript","title":"Welcome to InSARScript !","text":"<p>InSAR Script is an open-source package designed to support the full InSAR processing pipeline. The primary goal of this package is to provide a streamlined and user-friendly InSAR processing experience across multiple satellite products.</p> <p>To begin with the tutorial, click following buttons to:</p> <p>QuickStart</p> <p>This document will assume you have fundimental understanding of: </p> <ul> <li>Python </li> <li>Linux</li> <li>Conda</li> </ul>"},{"location":"#system-requirement","title":"System Requirement","text":"<p>InSARScript is designed to run on Unix-based systems. Internet connection is required.  Windows compatibility is under testing and may not function reliably.</p> <p>The package has been tested under Ubuntu 22.04.4 LTS </p> <p>Test environment:</p> Category Model CPU AMD Ryzen 7 7800x3d GPU Nvidia 4070 Super Memory 64GB DDR5 Operate System Ubuntu 22.04.4 LTS"},{"location":"#disclaimer","title":"Disclaimer","text":"<p>This package is provided \"as-is\" without any warranties, express or implied. The authors and contributors make no guarantees regarding the functionality, reliability, or suitability of the package for any particular purpose. By using this package, you assume all risks associated with its use, and the developers are not liable for any damages or issues that may arise, whether directly or indirectly, from its use.</p>"},{"location":"#need-help","title":"Need Help?","text":"<p>Meanwhile, if you have any question relate to the package or the documentation, please feel free to submit ,  join ,  or send me </p>"},{"location":"about/","title":"About","text":""},{"location":"about/#about","title":"About","text":""},{"location":"quickstart/analyzer/","title":"Analyzer","text":"<p>The InSARScript analyzer module provides workflow for InSAR time-series analysis.</p> <ul> <li> <p>Import analyzer</p> <p>Import the Analyzer class to access all time-series analysis functionality <pre><code>from insarscript import Analyzer\n</code></pre></p> </li> <li> <p>View Available Analyzers</p> <p>List all registered analyzer <pre><code>Analyzer.available()\n</code></pre></p> </li> </ul>"},{"location":"quickstart/analyzer/#available-analyzers","title":"Available Analyzers","text":""},{"location":"quickstart/analyzer/#mintpy_sbas_base_analyzer","title":"Mintpy_SBAS_Base_Analyzer","text":"<p>InSARScript wrapped Mintpy as one of it's analysis backends. The <code>Mintpy_SBAS_Base_Analyzer</code> is implemented on top of a reusable base configuration class, which provides the full <code>smallbaselineApp</code> logic of Mintpy. Provides users with an experience similar to using MintPy directly, allowing full customization of processing parameters and steps.</p> Source code in <code>src/insarscript/analyzer/mintpy_base.py</code> <pre><code>class Mintpy_SBAS_Base_Analyzer(BaseAnalyzer):\n\n    name = 'Mintpy_SBAS_Base_Analyzer'\n    default_config = Mintpy_SBAS_Base_Config\n    '''\n    Base class for Mintpy SBAS analysis. This class provides a template for implementing \n    specific analysis methods using the Mintpy software package.\n    '''\n    def __init__(self, config: Mintpy_SBAS_Base_Config | None = None):\n        super().__init__(config)\n\n        self.workdir = self.config.workdir\n        self.tmp_dir = self.workdir.joinpath('tmp')\n        self.clip_dir = self.workdir.joinpath('clip')\n        self.cfg_path = self.workdir.joinpath('mintpy.cfg')\n\n    def _cds_authorize(self):\n        if self._check_cdsapirc:\n           return True\n        else: \n            while True:\n                self._cds_token = getpass.getpass(\"Enter your CDS api token at https://cds.climate.copernicus.eu/profile: \")\n                cdsrc_path = Path.home().joinpath(\".cdsapirc\")\n                if cdsrc_path.is_file():\n                    cdsrc_path.unlink()\n                cdsrc_entry = f\"\\nurl: https://cds.climate.copernicus.eu/api\\nkey: {self._cds_token}\"\n                with open(cdsrc_path, 'a') as f:\n                    f.write(cdsrc_entry)\n                    print(f\"{Fore.GREEN}Credentials saved to {cdsrc_path}.\\n\")\n                try:\n                    tmp = (Path.home().joinpath(\".cdsrc_test\")).mkdir(exist_ok=True)\n                    pyaps3.ECMWFdload(['20200601','20200901'], hr='14', filedir=tmp, model='ERA5', snwe=(30,40,120,140))\n                    shutil.rmtree(tmp)\n                    print(f\"{Fore.GREEN}Authentication successful.\\n\")\n                except requests.exceptions.HTTPError as e:\n                    if e.response.status_code == 401:\n                        print(f'{Fore.RED} Authentication Failed please check your token')\n                break\n\n    def _check_cdsapirc(self):\n        \"\"\"Check if .cdsapirc token exist under home directory.\"\"\"\n        cdsapirc_path = Path.home().joinpath('.cdsapirc')\n        if not cdsapirc_path.is_file():            \n            print(f\"{Fore.RED}No .cdsapirc file found in your home directory. Will prompt login.\\n\")\n            return False\n        else: \n            with cdsapirc_path.open() as f:\n                content = f.read()\n                if 'key:' in content:\n                    return True\n                else:\n                    print(f\"{Fore.RED}no api token found under .cdsapirc. Will prompt login.\\n\")\n                    return False\n\n    def run(self, steps=None):\n        \"\"\"\n        Run the MintPy SBAS time-series analysis workflow.\n\n        This method writes the MintPy configuration file, optionally authorizes\n        CDS access for tropospheric correction, and executes the selected\n        MintPy processing steps using TimeSeriesAnalysis.\n\n        Args:\n            steps (list[str] | None, optional):\n                List of MintPy processing steps to execute. If None, the\n                default full workflow is executed:\n                    [\n                        'load_data', 'modify_network', 'reference_point',\n                        'invert_network', 'correct_LOD', 'correct_SET',\n                        'correct_ionosphere', 'correct_troposphere',\n                        'deramp', 'correct_topography', 'residual_RMS',\n                        'reference_date', 'velocity', 'geocode',\n                        'google_earth', 'hdfeos5'\n                    ]\n\n        Raises:\n            RuntimeError: If tropospheric delay method requires CDS authorization\n                and authorization fails.\n            Exception: Propagates exceptions raised during MintPy execution.\n\n        Notes:\n            - If `troposphericDelay_method` is set to 'pyaps', CDS\n            authorization is performed before running MintPy.\n            - The configuration file is written to `self.cfg_path`.\n            - Processing is executed inside `self.workdir`.\n            - This method wraps MintPy TimeSeriesAnalysis for SBAS workflows.\n        \"\"\"\n        if self.config.troposphericDelay_method == 'pyaps':\n            self._cds_authorize()\n        self.config.write_mintpy_config(self.cfg_path)\n\n        run_steps = steps or [\n            'load_data', 'modify_network', 'reference_point', 'invert_network',\n            'correct_LOD', 'correct_SET', 'correct_ionosphere', 'correct_troposphere',\n            'deramp', 'correct_topography', 'residual_RMS', 'reference_date',\n            'velocity', 'geocode', 'google_earth', 'hdfeos5'\n        ]\n        print(f'{Style.BRIGHT}{Fore.MAGENTA}Running MintPy Analysis...{Fore.RESET}')\n        app = TimeSeriesAnalysis(self.cfg_path.as_posix(), self.workdir.as_posix())\n        app.open()\n        app.run(steps=run_steps)\n\n    def cleanup(self):\n        \"\"\"\n        Remove temporary files and directories generated during processing.\n\n        This method deletes the temporary working directories and any `.zip`\n        archives in `self.workdir`. If debug mode is enabled, temporary files\n        are preserved and a message is printed instead.\n\n        Behavior:\n            - Deletes `self.tmp_dir` and `self.clip_dir` if they exist.\n            - Deletes all `.zip` files in `self.workdir`.\n            - Prints informative messages for each removal or failure.\n            - Respects `self.config.debug`; no files are deleted in debug mode.\n\n        Raises:\n            Exception: Propagates any unexpected errors raised during removal.\n\n        Notes:\n            - Useful for freeing disk space after large InSAR or MintPy\n            processing workflows.\n            - Temporary directories should contain only non-essential files\n            to avoid accidental data loss.\n        \"\"\"\n\n        if self.config.debug:\n            print(f\"{Fore.YELLOW}Debug mode is enabled. Keeping temporary files at: {self.workdir}{Fore.RESET}\")\n            return\n        print(f\"{Fore.CYAN}Step: Cleaning up temporary directories...{Fore.RESET}\")\n\n        for folder in [self.tmp_dir, self.clip_dir]:\n            if folder.exists() and folder.is_dir():\n                try:\n                    shutil.rmtree(folder)\n                    print(f\"  Removed: {folder.relative_to(self.workdir)}\")\n                except Exception as e:\n                    print(f\"{Fore.RED}  Failed to remove {folder}: {e}{Fore.RESET}\")\n\n        zips = list(self.workdir.glob('*.zip'))\n        if zips:\n            print(f\"{Fore.CYAN}Step: Removing zip archives...{Fore.RESET}\")\n            for zf in zips:\n                try:\n                    zf.unlink()\n                    print(f\"  Removed: {zf.name}\")\n                except Exception as e:\n                    print(f\"{Fore.RED}  Failed to remove {zf.name}: {e}{Fore.RESET}\")\n\n        print(f\"{Fore.GREEN}Cleanup complete.{Fore.RESET}\")\n</code></pre>"},{"location":"quickstart/analyzer/#usage","title":"Usage","text":"<ul> <li> <p>Create Analyzer with Parameters</p> <p>Initialize a analyzer instance</p> <p><pre><code>analyzer = Analyzer.create('Mintpy_SBAS_Base_Analyzer', \n                            workdir=\"/your/work/dir\",\n                            load_processor= \"hyp3\", ....)\n</code></pre> OR <pre><code>params = {\"workdir\":\"/your/work/dir\",\"load_processor\": \"hyp3\" ....}\nanalyzer = Analyzer.create('Mintpy_SBAS_Base_Analyzer', **params)\n</code></pre> OR</p> <pre><code>from insarscript.config import Mintpy_SBAS_Base_Config\ncfg = Mintpy_SBAS_Base_Config(workdir=\"/your/work/dir\",\n                              load_processor= \"hyp3\",\n                              ....)\nanalyzer = Analyzer.create('Mintpy_SBAS_Base_Analyzer', config=cfg)\n</code></pre> <p>The base configure <code>Mintpy_SBAS_Base_Config</code> contains all parameters from Mintpy <code>smallbaselineApp.cfg</code>. For detailed descriptions and usage of each parameters, please refer to the official Mintpy config documentation. </p> Source code in <code>src/insarscript/config/defaultconfig.py</code> <pre><code>@dataclass\nclass Mintpy_SBAS_Base_Config:\n    '''\n    Dataclass containing all configuration options for Mintpy SBAS jobs.\n    '''\n    name: str = \"Mintpy_SBAS_Base_Config\"\n    workdir: Path | str = field(default_factory=lambda: Path.cwd())\n    debug: bool = False \n\n    ## computing resource configuration\n    compute_maxMemory : float | int = _env['memory']\n    compute_cluster : str = 'local' # Mintpy's slurm parallel processing is buggy, so we will handle parallel processing with dask instead. Switch to none to turn off parallel processing to save memory.\n    compute_numWorker : int = _env['cpu']\n    compute_config: str = 'none'\n\n    ## Load data\n    load_processor: str = 'auto'\n    load_autoPath: str = 'auto' \n    load_updateMode: str = 'auto'\n    load_compression: str = 'auto'\n    ##---------for ISCE only:\n    load_metaFile: str = 'auto'\n    load_baselineDir: str = 'auto'\n    ##---------interferogram stack:\n    load_unwFile: str = 'auto'\n    load_corFile: str = 'auto'\n    load_connCompFile: str = 'auto'\n    load_intFile: str = 'auto'\n    load_magFile: str = 'auto'\n    ##---------ionosphere stack (optional):\n    load_ionUnwFile: str = 'auto'\n    load_ionCorFile: str = 'auto'\n    load_ionConnCompFile: str = 'auto'\n    ##---------offset stack (optional):\n    load_azOffFile: str = 'auto'\n    load_rgOffFile: str = 'auto'\n    load_azOffStdFile: str = 'auto'\n    load_rgOffStdFile: str = 'auto'\n    load_offSnrFile: str = 'auto'\n    ##---------geometry:\n    load_demFile: str = 'auto'\n    load_lookupYFile: str = 'auto'\n    load_lookupXFile: str = 'auto'\n    load_incAngleFile: str = 'auto'\n    load_azAngleFile: str = 'auto'\n    load_shadowMaskFile: str = 'auto'\n    load_waterMaskFile: str = 'auto'\n    load_bperpFile: str = 'auto'\n    ##---------subset (optional):\n    subset_yx: str = 'auto'\n    subset_lalo: str = 'auto'\n    ##---------multilook (optional):\n    multilook_method: str = 'auto'\n    multilook_ystep: str | int = 'auto'\n    multilook_xstep: str | int= 'auto'\n\n    # 2. Modify Network\n    network_tempBaseMax: str | float = 'auto'\n    network_perpBaseMax: str | float = 'auto'\n    network_connNumMax: str | int = 'auto'\n    network_startDate: str = 'auto'\n    network_endDate: str = 'auto'\n    network_excludeDate: str = 'auto'\n    network_excludeDate12: str = 'auto'\n    network_excludeIfgIndex: str = 'auto'\n    network_referenceFile: str = 'auto'\n    ## 2) Data-driven network modification\n    network_coherenceBased: str = 'auto'\n    network_minCoherence: str |float = 'auto'\n    ## b - Effective Coherence Ratio network modification = (threshold + MST) by default\n    network_areaRatioBased: str = 'auto'\n    network_minAreaRatio: str |float= 'auto'\n    ## Additional common parameters for the 2) data-driven network modification\n    network_keepMinSpanTree: str = 'auto'\n    network_maskFile: str = 'auto'\n    network_aoiYX: str = 'auto'\n    network_aoiLALO: str = 'auto'\n\n    # 3. Reference Point\n    reference_yx: str = 'auto'\n    reference_lalo: str = 'auto'\n    reference_maskFile: str = 'auto'\n    reference_coherenceFile: str = 'auto'\n    reference_minCoherence: str |float = 'auto'\n\n    # 4. Correct Unwrap Error\n    unwrapError_method: str = 'auto'\n    unwrapError_waterMaskFile: str = 'auto'\n    unwrapError_connCompMinArea: str |float = 'auto'\n    ## phase_closure options:\n    unwrapError_numSample: str | int= 'auto'\n    ## bridging options:\n    unwrapError_ramp: str = 'auto'\n    unwrapError_bridgePtsRadius: str | int= 'auto'\n\n    # 5. Invert Network\n    networkInversion_weightFunc: str = 'auto'\n    networkInversion_waterMaskFile: str = 'auto'\n    networkInversion_minNormVelocity: str = 'auto'\n    ## mask options for unwrapPhase of each interferogram before inversion (recommend if weightFunct=no):\n    networkInversion_maskDataset: str = 'auto'\n    networkInversion_maskThreshold: str | float = 'auto'\n    networkInversion_minRedundancy: str | float = 'auto'\n    ## Temporal coherence is calculated and used to generate the mask as the reliability measure\n    networkInversion_minTempCoh: str | float = 'auto'\n    networkInversion_minNumPixel: str | int = 'auto'\n    networkInversion_shadowMask: str = 'auto'\n\n    # 6. Correct SET (Solid Earth Tides)\n    solidEarthTides: str = 'auto'\n\n    # 7. Correct Ionosphere\n    ionosphericDelay_method: str = 'auto'\n    ionosphericDelay_excludeDate: str = 'auto'\n    ionosphericDelay_excludeDate12: str = 'auto'\n\n    # 8. Correct Troposphere\n    troposphericDelay_method: str = 'auto'\n    ## Notes for pyaps:\n    troposphericDelay_weatherModel: str = 'auto'\n    troposphericDelay_weatherDir: str = 'auto'\n\n    ## Notes for height_correlation:\n    troposphericDelay_polyOrder: str | int = 'auto'\n    troposphericDelay_looks: str | int = 'auto'\n    troposphericDelay_minCorrelation: str | float = 'auto'\n    ## Notes for gacos:\n    troposphericDelay_gacosDir: str = 'auto'\n\n    # 9. Deramp\n    deramp: str = 'auto'\n    deramp_maskFile: str = 'auto'\n\n    # 10. Correct Topography\n    topographicResidual: str = 'auto'\n    topographicResidual_polyOrder: str = 'auto'\n    topographicResidual_phaseVelocity: str = 'auto'\n    topographicResidual_stepDate: str = 'auto'\n    topographicResidual_excludeDate: str = 'auto'\n    topographicResidual_pixelwiseGeometry: str = 'auto'\n\n    # 11.1 Residual RMS\n    residualRMS_maskFile: str = 'auto'\n    residualRMS_deramp: str = 'auto'\n    residualRMS_cutoff: str | float = 'auto'\n\n    # 11.2 Reference Date\n    reference_date: str = 'auto'\n\n    # 12. Velocity\n    timeFunc_startDate: str = 'auto'\n    timeFunc_endDate: str = 'auto'\n    timeFunc_excludeDate: str = 'auto'\n    ## Fit a suite of time functions\n    timeFunc_polynomial: str | int = 'auto'\n    timeFunc_periodic: str = 'auto'\n    timeFunc_stepDate: str = 'auto'\n    timeFunc_exp: str = 'auto'\n    timeFunc_log: str = 'auto'\n    ## Uncertainty quantification methods:\n    timeFunc_uncertaintyQuantification: str = 'auto'\n    timeFunc_timeSeriesCovFile: str = 'auto'\n    timeFunc_bootstrapCount: str | int = 'auto'\n\n    # 13.1 Geocode\n    geocode: str = 'auto'\n    geocode_SNWE: str = 'auto'\n    geocode_laloStep: str = 'auto'\n    geocode_interpMethod: str = 'auto'\n    geocode_fillValue: str | float = 'auto'\n\n    # 13.2 Google Earth\n    save_kmz: str = 'auto'\n\n    # 13.3 HDFEOS5\n    save_hdfEos5: str = 'auto'\n    save_hdfEos5_update: str = 'auto'\n    save_hdfEos5_subset: str = 'auto'\n\n    # 13.4 Plot\n    plot: str = 'auto'\n    plot_dpi: str | int = 'auto'\n    plot_maxMemory: str | int = 'auto'\n\n    def __post_init__(self):\n        if isinstance(self.workdir, str):\n            self.workdir = Path(self.workdir).expanduser().resolve()\n\n    def write_mintpy_config(self, outpath: Union[Path, str]):\n        \"\"\"\n        Writes the dataclass to a mintpy .cfg file, excluding operational \n        parameters that MintPy doesn't recognize.\n        \"\"\"\n        outpath = Path(outpath).expanduser().resolve()\n        exclude_fields = ['name', 'workdir', 'debug']\n\n        with open(outpath, 'w') as f:\n            f.write(\"## MintPy Config File Generated via InSARScript\\n\")\n\n            for key, value in asdict(self).items():\n                if key in exclude_fields:\n                    continue\n\n                parts = key.split('_')\n                if len(parts) &gt; 1:\n                    mintpy_key = f\"mintpy.{parts[0]}.{'.'.join(parts[1:])}\"\n                else:\n                    mintpy_key = f\"mintpy.{parts[0]}\"\n\n                f.write(f\"{mintpy_key:&lt;40} = {value}\\n\")\n\n        return Path(outpath).resolve()\n</code></pre> </li> <li> <p>Run     Run the Mintpy time-series analysis based on provid configuration</p> <pre><code>analyzer.run()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>list[str] | None</code> <p>List of MintPy processing steps to execute. If None, the default full workflow is executed:     [         'load_data', 'modify_network', 'reference_point',         'invert_network', 'correct_LOD', 'correct_SET',         'correct_ionosphere', 'correct_troposphere',         'deramp', 'correct_topography', 'residual_RMS',         'reference_date', 'velocity', 'geocode',         'google_earth', 'hdfeos5'     ]</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If tropospheric delay method requires CDS authorization and authorization fails.</p> <code>Exception</code> <p>Propagates exceptions raised during MintPy execution.</p> </li> <li> <p>Clean up</p> <p>Remove intermediate processing files generated during the time-series process</p> <pre><code>analyzer.cleanup()\n</code></pre> <p>Raises:</p> Type Description <code>Exception</code> <p>Propagates any unexpected errors raised during removal.</p> </li> </ul>"},{"location":"quickstart/analyzer/#hyp3_sbas","title":"Hyp3_SBAS","text":"<p>The <code>Hyp3_SBAS</code> is specialized analyzer that extends Mintpy_SBAS_Base_Analyzer, preconfigured specifically for processing Time-series data for Hyp3 InSAR product.</p> Source code in <code>src/insarscript/analyzer/hyp3_sbas.py</code> <pre><code>class Hyp3_SBAS(Mintpy_SBAS_Base_Analyzer):\n    name = 'Hyp3_SBAS'\n    default_config = Hyp3_SBAS_Config\n    required = ['unw_phase.tif', 'corr.tif',  'dem.tif'] # also need meta files to get the date and other info\n    optional = ['lv_theta.tif', 'lv_phi.tif', 'water_mask.tif']\n    def __init__(self, config: Hyp3_SBAS_Config | None = None):\n        super().__init__(config)\n\n    def prep_data(self):\n        \"\"\"\n        Prepare input data for analysis by performing unzipping, collection, clipping, and parameter setup.\n\n        This method orchestrates the preprocessing steps required before running the analysis workflow. \n        It ensures that all input files are available, aligned, and properly configured.\n\n        Steps performed:\n            1. `_unzip_hyp3()`: Extracts any compressed Hyp3 output files.\n            2. `_collect_files()`: Gathers relevant input files (e.g., DEMs, interferograms).\n            3. `_get_common_overlap(files['dem'])`: Computes the spatial overlap extent among input rasters.\n            4. `_clip_rasters(files, overlap_extent)`: Clips input rasters to the common overlapping area.\n            5. `_set_load_parameters()`: Sets parameters required for loading the preprocessed data into memory.\n\n        Raises:\n            FileNotFoundError: If required input files are missing.\n            ValueError: If no common overlap region can be determined among rasters.\n            Exception: Propagates any unexpected errors during preprocessing.\n\n        Notes:\n            - This method must be called before running the analysis workflow.\n            - Designed for workflows using Hyp3-derived Sentinel-1 products.\n            - Ensures consistent spatial coverage across all input datasets.\n        \"\"\"\n        self._unzip_hyp3()\n        files = self._collect_files()\n        overlap_extent = self._get_common_overlap(files['dem'])\n        self._clip_rasters(files, overlap_extent)\n        self._set_load_parameters()\n\n    def _unzip_hyp3(self):\n        print(f'{Fore.CYAN}Unzipping HyP3 Products...{Fore.RESET}')\n\n        hyp3_results = list(self.workdir.rglob('*.zip'))\n        self.tmp_dir.mkdir(parents=True, exist_ok=True)\n\n        with tqdm(hyp3_results, desc=\"Processing\", unit=\"file\") as pbar:\n            for zip_file in pbar:\n                extract_target = self.tmp_dir / zip_file.stem\n                with zipfile.ZipFile(zip_file, 'r') as zf:\n                    needs_extraction = True\n                    if extract_target.is_dir():\n                        files_in_zip = {Path(f).name for f in zf.namelist() if not f.endswith('/')}\n                        folder_files = {f.name for f in extract_target.iterdir() if f.is_file()}\n                        if files_in_zip.issubset(folder_files):\n                            needs_extraction = False\n                            pbar.set_description(f\"File Exist: {zip_file.stem[:30]}...\")\n                    if needs_extraction:\n                        pbar.set_description(f\"Extracting: {zip_file.stem[:30]}...\")\n                        if extract_target.is_dir():\n                            shutil.rmtree(extract_target)\n\n                        zf.extractall(self.tmp_dir)\n        print(f'\\n{Fore.GREEN}Unzipping complete.{Fore.RESET}')\n\n    def _collect_files(self):\n        print(f'{Fore.CYAN}Mapping file paths...{Fore.RESET}')\n        all_required = {ext.split('.')[0]: ext for ext in self.required}    \n        all_optional = {ext.split('.')[0]: ext for ext in self.optional}\n        files = defaultdict(list)\n        files['meta'] = [m for m in self.tmp_dir.rglob('*.txt') if 'README' not in m.name]\n        for cat_name, ext in {**all_required, **all_optional}.items():\n            files[cat_name] = list(self.tmp_dir.rglob(f\"*_{ext}\"))\n\n        missing_req = [name for name, ext in all_required.items() if not files[name]]\n        if missing_req or not files['meta']:\n            print(f\"\\033[K\", end=\"\\r\") # Clear current line\n            msg = []\n            if missing_req: msg.append(f\"Missing rasters: {missing_req}\")\n            if not files['meta']: msg.append(\"Missing metadata (.txt) files\")\n\n            error_report = f\"{Fore.RED}CRITICAL ERROR: {'. '.join(msg)}.{Fore.RESET}\\n\" \\\n                           f\"MintPy requires these files to extract dates and baselines.\"\n            raise FileNotFoundError(error_report)\n        missing_opt = [name for name in all_optional if not files[name]]\n\n        total_pairs = len(files['unw_phase'])\n        status_msg = f\"{Fore.GREEN}Found {total_pairs} pairs | Metadata: OK\"\n        if missing_opt:\n            status_msg += f\" | {Fore.YELLOW}Missing optional: {missing_opt}\"\n\n        print(f\"\\r\\033[K{status_msg}{Fore.RESET}\")\n        return files\n\n    def _get_common_overlap(self, dem_files):\n        ulx_l, uly_l, lrx_l, lry_l = [], [], [], []\n        for f in dem_files:\n            ds = gdal.Open(f.as_posix())\n            gt = ds.GetGeoTransform() # (ulx, xres, xrot, uly, yrot, yres)\n            ulx, uly = gt[0], gt[3]\n            lrx, lry = gt[0] + gt[1] * ds.RasterXSize, gt[3] + gt[5] * ds.RasterYSize\n            ulx_l.append(ulx)\n            uly_l.append(uly)\n            lrx_l.append(lrx)\n            lry_l.append(lry)\n            ds = None\n        return  (max(ulx_l), min(uly_l), min(lrx_l), max(lry_l))\n\n    def _clip_rasters(self, files, overlap_extent):\n        print(f'{Fore.CYAN}Clipping rasters to common overlap...{Fore.RESET}')\n        self.clip_dir.mkdir(parents=True, exist_ok=True)\n        categories = [k for k in files.keys() if k != 'meta']\n\n        with tqdm(categories, desc=\"Progress\", position=0, dynamic_ncols=True) as pbar_out:\n            for key in pbar_out:\n                file_list = files[key]\n                pbar_out.set_description(f\"Group: {key}\")\n\n                # Inner progress bar for individual files in this group\n                # leave=False ensures the inner bar disappears when the group is done\n                with tqdm(file_list, desc=f\"  -&gt; Clipping\", leave=False, position=1, unit=\"file\", dynamic_ncols=True) as pbar_in:\n                    for f in pbar_in:\n                        out = self.clip_dir / f\"{f.stem}_clip.tif\"\n\n                        if out.exists():\n                            pbar_in.set_postfix_str(f\"Skip: {f.name[:15]}...\")\n                            # Update postfix instead of printing to avoid creating new lines\n                            continue\n\n                        pbar_in.set_postfix_str(f\"File: {f.name[:15]}...\")\n\n                        try:\n                            gdal.Translate(\n                                destName=out.as_posix(),\n                                srcDS=f.as_posix(),\n                                projWin=overlap_extent\n                            )\n                        except Exception as e:\n                            tqdm.write(f\"{Fore.RED}Error clipping {f.name}: {e}{Fore.RESET}\")\n\n            # Handle metadata separately as it's just a file copy (no progress bar needed)\n        if 'meta' in files:\n            print(f\"\\r{Fore.CYAN}Step: Copying metadata files... \\033[K\", end=\"\", flush=True)\n            for f in files['meta']:\n                shutil.copy(f, self.clip_dir / f.name)\n\n        print(f'\\n{Fore.GREEN}Clipping complete.{Fore.RESET}')\n\n    def _set_load_parameters(self):\n        self.config.load_unwFile = (self.clip_dir / '*_unw_phase_clip.tif').as_posix()\n        self.config.load_corFile = (self.clip_dir / '*_corr_clip.tif').as_posix()\n        self.config.load_demFile = (self.clip_dir / '*_dem_clip.tif').as_posix()\n        opt_map = {\n            'lv_theta': 'load_incAngleFile',\n            'lv_phi': 'load_azAngleFile',\n            'water_mask': 'load_waterMaskFile'\n        }\n        for k, cfg_attr in opt_map.items():\n            if list(self.clip_dir.glob(f\"*_{k}_clip.tif\")):\n                setattr(self.config, cfg_attr, (self.clip_dir / f\"*_{k}_clip.tif\").as_posix())\n</code></pre>"},{"location":"quickstart/analyzer/#usage_1","title":"Usage","text":"<ul> <li> <p>Create Analyzer with Parameters</p> <p>Initialize a analyzer instance</p> <p><pre><code>analyzer = Analyzer.create('Hyp3_SBAS', \n                            workdir=\"/your/work/dir\")\n</code></pre> OR <pre><code>params = {\"workdir\":\"/your/work/dir\"}\nanalyzer = Analyzer.create('Hyp3_SBAS', **params)\n</code></pre> OR</p> <pre><code>from insarscript.config import Mintpy_SBAS_Base_Config\ncfg = Mintpy_SBAS_Base_Config(workdir=\"/your/work/dir\")\nanalyzer = Analyzer.create('Hyp3_SBAS', config=cfg)\n</code></pre> <ul> <li>Prepare data Prepare interferogram data download from hyp3 server to mintpy</li> </ul> <pre><code>analyzer.prep_data()\n</code></pre> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If required input files are missing.</p> <code>ValueError</code> <p>If no common overlap region can be determined among rasters.</p> <code>Exception</code> <p>Propagates any unexpected errors during preprocessing.</p> Source code in <code>src/insarscript/analyzer/hyp3_sbas.py</code> <pre><code>def prep_data(self):\n    \"\"\"\n    Prepare input data for analysis by performing unzipping, collection, clipping, and parameter setup.\n\n    This method orchestrates the preprocessing steps required before running the analysis workflow. \n    It ensures that all input files are available, aligned, and properly configured.\n\n    Steps performed:\n        1. `_unzip_hyp3()`: Extracts any compressed Hyp3 output files.\n        2. `_collect_files()`: Gathers relevant input files (e.g., DEMs, interferograms).\n        3. `_get_common_overlap(files['dem'])`: Computes the spatial overlap extent among input rasters.\n        4. `_clip_rasters(files, overlap_extent)`: Clips input rasters to the common overlapping area.\n        5. `_set_load_parameters()`: Sets parameters required for loading the preprocessed data into memory.\n\n    Raises:\n        FileNotFoundError: If required input files are missing.\n        ValueError: If no common overlap region can be determined among rasters.\n        Exception: Propagates any unexpected errors during preprocessing.\n\n    Notes:\n        - This method must be called before running the analysis workflow.\n        - Designed for workflows using Hyp3-derived Sentinel-1 products.\n        - Ensures consistent spatial coverage across all input datasets.\n    \"\"\"\n    self._unzip_hyp3()\n    files = self._collect_files()\n    overlap_extent = self._get_common_overlap(files['dem'])\n    self._clip_rasters(files, overlap_extent)\n    self._set_load_parameters()\n</code></pre> <ul> <li>Run Run the Mintpy time-series analysis based on provid configuration</li> </ul> <pre><code>analyzer.run()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>list[str] | None</code> <p>List of MintPy processing steps to execute. If None, the default full workflow is executed:     [         'load_data', 'modify_network', 'reference_point',         'invert_network', 'correct_LOD', 'correct_SET',         'correct_ionosphere', 'correct_troposphere',         'deramp', 'correct_topography', 'residual_RMS',         'reference_date', 'velocity', 'geocode',         'google_earth', 'hdfeos5'     ]</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If tropospheric delay method requires CDS authorization and authorization fails.</p> <code>Exception</code> <p>Propagates exceptions raised during MintPy execution.</p> Source code in <code>src/insarscript/analyzer/mintpy_base.py</code> <pre><code>def run(self, steps=None):\n    \"\"\"\n    Run the MintPy SBAS time-series analysis workflow.\n\n    This method writes the MintPy configuration file, optionally authorizes\n    CDS access for tropospheric correction, and executes the selected\n    MintPy processing steps using TimeSeriesAnalysis.\n\n    Args:\n        steps (list[str] | None, optional):\n            List of MintPy processing steps to execute. If None, the\n            default full workflow is executed:\n                [\n                    'load_data', 'modify_network', 'reference_point',\n                    'invert_network', 'correct_LOD', 'correct_SET',\n                    'correct_ionosphere', 'correct_troposphere',\n                    'deramp', 'correct_topography', 'residual_RMS',\n                    'reference_date', 'velocity', 'geocode',\n                    'google_earth', 'hdfeos5'\n                ]\n\n    Raises:\n        RuntimeError: If tropospheric delay method requires CDS authorization\n            and authorization fails.\n        Exception: Propagates exceptions raised during MintPy execution.\n\n    Notes:\n        - If `troposphericDelay_method` is set to 'pyaps', CDS\n        authorization is performed before running MintPy.\n        - The configuration file is written to `self.cfg_path`.\n        - Processing is executed inside `self.workdir`.\n        - This method wraps MintPy TimeSeriesAnalysis for SBAS workflows.\n    \"\"\"\n    if self.config.troposphericDelay_method == 'pyaps':\n        self._cds_authorize()\n    self.config.write_mintpy_config(self.cfg_path)\n\n    run_steps = steps or [\n        'load_data', 'modify_network', 'reference_point', 'invert_network',\n        'correct_LOD', 'correct_SET', 'correct_ionosphere', 'correct_troposphere',\n        'deramp', 'correct_topography', 'residual_RMS', 'reference_date',\n        'velocity', 'geocode', 'google_earth', 'hdfeos5'\n    ]\n    print(f'{Style.BRIGHT}{Fore.MAGENTA}Running MintPy Analysis...{Fore.RESET}')\n    app = TimeSeriesAnalysis(self.cfg_path.as_posix(), self.workdir.as_posix())\n    app.open()\n    app.run(steps=run_steps)\n</code></pre> <ul> <li>Clean up</li> </ul> <p>Remove intermediate processing files generated during the time-series process</p> <pre><code>analyzer.cleanup()\n</code></pre> <p>Raises:</p> Type Description <code>Exception</code> <p>Propagates any unexpected errors raised during removal.</p> </li> </ul>"},{"location":"quickstart/api_setup/","title":"API Setup","text":"<p>This program requires several API accounts for full functionality. Registration for all of these services is completely free.</p>"},{"location":"quickstart/api_setup/#nasa-earthdata","title":"NASA Earthdata","text":"<p>This account is required for searching satellite scenes, downloading DEMs and orbit files, and submitting online interferogram processing jobs.</p> <p>Once the registration is complete create a file named <code>.netrc</code> under your home directory if not exist and add  <pre><code>machine urs.earthdata.nasa.gov\n    login Your_Earthdata_username\n    password Your_Earthdata_password\n</code></pre> <code>OR</code></p> <p>The program will prompts for login on first use.</p>"},{"location":"quickstart/api_setup/#copernicus-data-space-ecosystem","title":"Copernicus Data Space Ecosystem","text":"<p>This account is required for downloading orbit files, the release of orbit files on this website is faster than NASA earthdata for couple hours to days. </p> <p>Once the registration is complete create a file named <code>.netrc</code> under your home directory if not exist and add </p> <pre><code>machine dataspace.copernicus.eu\n    login Your_CDSE_username\n    password Your_CDSE_password \n</code></pre> <p><code>OR</code> </p> <p>The program will prompts for login on first use.</p>"},{"location":"quickstart/api_setup/#copernicus-climate-data-store","title":"Copernicus Climate Data Store","text":"<p>This account is required to perform tropospheric correction using PyAPS.</p> <p>Once the registration is complete create a file named <code>.cdsapirc</code> under your home directory if not exist and add you API Token:</p> <pre><code>url: https://cds.climate.copernicus.eu/api\nkey: your-personal-access-token\n</code></pre>"},{"location":"quickstart/downloader/","title":"Downloader","text":"<p>The InSARScript Downloader module provides a streamlined interface for searching and downloading satellite data.</p> <ul> <li> <p>Import downloader</p> <p>Import the Downloader class to access all downloader functionality <pre><code>from insarscript import Downloader\n</code></pre></p> </li> <li> <p>View available downloaders</p> <p>List all registered downloader <pre><code>Downloader.available()\n</code></pre></p> </li> </ul>"},{"location":"quickstart/downloader/#available-downloaders","title":"Available Downloaders","text":""},{"location":"quickstart/downloader/#asf-base-downloader","title":"ASF Base Downloader","text":"<p>InSARScript wrapped asf_search as one of its download backends. The <code>ASF_Base_Downloader</code> is implemented on top of a reusable base configuration class, which provides the full searching, filtering, and downloading logic of asf_search.</p> Source code in <code>src/insarscript/downloader/asf_base.py</code> <pre><code>class ASF_Base_Downloader(BaseDownloader): \n    \"\"\"\n    Simplify searching and downloading satellite data using ASF Search API.\n    \"\"\"\n    name = \"ASF_base_Downloader\"\n    default_config = ASF_Base_Config\n    _DATASET_GROUP_KEYS = {\n        'SENTINEL-1': ('pathNumber', 'frameNumber'),\n        'ALOS':       ('pathNumber', 'frameNumber'),\n        'NISAR':      ('pathNumber', 'frameID'),\n        'BURST':      ('pathNumber', 'burstID'),\n    }\n    _DATASET_PROPERTY_KEYS = {\n        'SENTINEL-1': {\n            'relativeOrbit': 'pathNumber',\n            'absoluteOrbit': 'absoluteOrbit',\n            'polarization':  'polarization',\n            'flightDirection': 'flightDirection',\n        },\n        'ALOS': {\n            'relativeOrbit': 'pathNumber',\n            'absoluteOrbit': 'absoluteOrbit',\n            'polarization':  'polarization',\n            'flightDirection': 'flightDirection',\n        },\n        'NISAR': {\n            'relativeOrbit': 'relativeOrbit',\n            'absoluteOrbit': 'absoluteOrbit',\n            'polarization':  'polarization',\n            'flightDirection': 'flightDirection',\n        },\n    }\n\n    def __init__(self, config: ASF_Base_Config | None = None): \n\n        \"\"\"\n        Initialize the Downloader with search parameters. Options was adapted from asf_search searching api. \n        You may check https://docs.asf.alaska.edu/asf_search/searching/ for more info, below only list customized parameters.\n        \"\"\"\n        print(f\"\"\"\nThis downloader relies on the ASF API. Please ensure you to create an account at https://search.asf.alaska.edu/. \nIf a .netrc file is not provide under your home directory, you will be prompt to enter your ASF username and password. \nCheck documentation for how to setup .netrc file.\\n\"\"\")\n        super().__init__(config)\n\n        if self.config.dataset is None and self.config.platform is None:\n            raise ValueError(f\"{Fore.RED}Dataset or platform must be specified for ASF search.\")\n\n        self.config.intersectsWith = _to_wkt(self.config.intersectsWith)\n\n\n    def _asf_authorize(self):\n        self._has_asf_netrc = self._check_netrc(keyword='machine urs.earthdata.nasa.gov')\n        if not self._has_asf_netrc:\n            while True:\n                _username = input(\"Enter your ASF username: \")\n                _password = getpass.getpass(\"Enter your ASF password: \")\n                try:\n                    self._session = asf.ASFSession().auth_with_creds(_username, _password)\n                except ASFAuthenticationError:\n                    print(f\"{Fore.RED}Authentication failed. Please check your credentials and try again.\\n\")\n                    continue\n                print(f\"{Fore.GREEN}Authentication successful.\\n\")\n                netrc_path = Path.home().joinpath(\".netrc\")\n                asf_entry = f\"\\nmachine urs.earthdata.nasa.gov\\n    login {_username}\\n    password {_password}\\n\"\n                with open(netrc_path, 'a') as f:\n                    f.write(asf_entry)\n                print(f\"{Fore.GREEN}Credentials saved to {netrc_path}. You can now use the downloader without entering credentials again.\\n\")\n                break\n        else:\n            self._session = asf.ASFSession()\n\n    def _check_netrc(self, keyword: str) -&gt; bool:\n        \"\"\"Check if .netrc file exists in the home directory with the specified keyword.\n\n        Args:\n            keyword (str): The machine name to search for in .netrc file.\n\n        Returns:\n            bool: True if .netrc file exists and contains the keyword, False otherwise.\n        \"\"\"\n        netrc_path = Path.home().joinpath('.netrc')\n        if not netrc_path.is_file():            \n            print(f\"{Fore.RED}No .netrc file found in your home directory. Will prompt login.\\n\")\n            return False\n        else: \n            with netrc_path.open() as f:\n                content = f.read()\n                if keyword in content:\n                    return True\n                else:\n                    print(f\"{Fore.RED}no machine name {keyword} found .netrc file. Will prompt login.\\n\")\n                    return False\n\n\n    def _get_group_key(self, result) -&gt; tuple:\n        \"\"\"Derive grouping key based on available properties, with fallback.\n\n        Args:\n            result: Search result object containing properties.\n\n        Returns:\n            tuple: A tuple of (path_number, frame_identifier) used for grouping results.\n        \"\"\"\n        props = result.properties\n        # Burst product \u2014 any burst ID field set in config takes highest priority\n        if any([\n            self.config.absoluteBurstID,\n            self.config.fullBurstID,\n            self.config.operaBurstID,\n            self.config.relativeBurstID,\n        ]):\n            return (props.get('pathNumber'), props.get('burstID'))\n\n        if self.config.asfFrame is not None:\n            return (props.get('pathNumber'), props.get('asfFrame'))\n\n        if self.config.frame is not None:\n            return (props.get('pathNumber'), props.get('frameNumber'))\n\n        # Dataset-level mapping\n        if self.config.dataset:\n            datasets = [self.config.dataset] if isinstance(self.config.dataset, str) else self.config.dataset\n            for ds in datasets:\n                ds_upper = ds.upper()\n                if ds_upper in self._DATASET_GROUP_KEYS:\n                    pk, fk = self._DATASET_GROUP_KEYS[ds_upper]\n                    return (props.get(pk), props.get(fk))\n        # Platform-level fallback mapping      \n        if self.config.platform:\n            platforms = [self.config.platform] if isinstance(self.config.platform, str) else self.config.platform\n            for pl in platforms:\n                pl_upper = pl.upper()\n                if 'SENTINEL' in pl_upper:\n                    return (props.get('pathNumber'), props.get('frameNumber'))\n                if 'ALOS' in pl_upper:\n                    return (props.get('pathNumber'), props.get('frameNumber'))\n                if 'NISAR' in pl_upper:\n                    return (props.get('pathNumber'), props.get('frameID'))\n        # last resort \u2014 group everything under the platform name\n        return (props.get('pathNumber'), props.get('frameNumber'))\n\n    def _get_property_keys(self) -&gt; dict:\n        \"\"\"Return the correct result.properties key mapping based on config.\n\n        Returns:\n            dict: Mapping of property names to their corresponding keys in search results.\n        \"\"\"\n        if self.config.dataset:\n            datasets = [self.config.dataset] if isinstance(self.config.dataset, str) else self.config.dataset\n            for ds in datasets:\n                ds_upper = ds.upper()\n                if ds_upper in self._DATASET_PROPERTY_KEYS:\n                    return self._DATASET_PROPERTY_KEYS[ds_upper]\n\n        if self.config.platform:\n            platforms = [self.config.platform] if isinstance(self.config.platform, str) else self.config.platform\n            for pl in platforms:\n                if 'SENTINEL' in pl.upper():\n                    return self._DATASET_PROPERTY_KEYS['SENTINEL-1']\n                if 'ALOS' in pl.upper():\n                    return self._DATASET_PROPERTY_KEYS['ALOS']\n                if 'NISAR' in pl.upper():\n                    return self._DATASET_PROPERTY_KEYS['NISAR']\n\n        # Default to Sentinel-1 keys as they are most common\n        return self._DATASET_PROPERTY_KEYS['SENTINEL-1']\n\n    @property\n    def session(self):\n        \"\"\"Get or create an authenticated ASF session.\n\n        Returns:\n            ASFSession: Authenticated session for ASF downloads.\n        \"\"\"\n        if not hasattr(self, '_session') or self._session is None:\n            self._asf_authorize()\n        return self._session\n\n    @property\n    def active_results(self):\n        \"\"\"Get the currently active results (filtered or full search results).\n\n        Returns the subset of results if a filter/pick is active, \n        otherwise returns the full search results.\n\n        Returns:\n            dict: Dictionary of active search results grouped by (path, frame).\n\n        Raises:\n            ValueError: If no search results are available.\n        \"\"\"\n        if not hasattr(self, 'results'):\n             raise ValueError(f\"{Fore.RED}No search results found. Please run search() first.\")\n        return self._subset if self._subset is not None else self.results\n\n    def search(self) -&gt; dict:\n        \"\"\"Search for data using the ASF Search API with the provided parameters.\n\n        Returns:\n            dict: Dictionary of search results grouped by (path, frame) tuples.\n\n        Raises:\n            ValueError: If search returns no results.\n            Exception: If search fails after 10 retry attempts.\n        \"\"\"\n        self._subset = None\n        print(f\"Searching for SLCs....\")\n        search_opts = {k: v for k, v in asdict(self.config).items() \n                       if v is not None and k not in ['workdir', 'name', 'bbox']}\n\n        for attempt in range(1, 11):\n            try:\n                self.results = asf.search(**search_opts)\n                break\n            except Exception as e:\n                print(f\"{Fore.RED}Search failed: {e}\")\n                if attempt == 10:\n                    raise\n                time.sleep(2 ** attempt)  \n\n        if not self.results:\n            raise ValueError(f'{Fore.RED}Search does not return any result, please check input parameters or Internet connection')\n        else:\n            print(f\"{Fore.GREEN} -- A total of {len(self.results)} results found. \\n\")\n\n        grouped = defaultdict(list)\n        for result in self.results:\n            key = self._get_group_key(result)\n            grouped[key].append(result)\n        self.results = grouped\n        if len(grouped) &gt; 1: \n            print(f\"{Fore.YELLOW}The AOI crosses {len(grouped)} stacks, you can use .summary() or .footprint() to check footprints and .pick((path_frame)) to specific the stack of scence you would like to download. If use .download() directly will create subfolders under {self.config.workdir} for each stack\")\n        return grouped\n\n    def reset(self):\n        \"\"\"Reset the view to include all search results.\n\n        Clears any active filters and restores the full result set.\n        \"\"\"\n        self._subset = None\n        print(f\"{Fore.GREEN}Selection reset. Now viewing all {len(self.results)} stacks.\")\n\n    def summary(self, ls=False):\n        \"\"\"Summarize the active results, separated by flight direction.\n\n        Args:\n            ls (bool, optional): If True, list individual scene names and dates. \n                Defaults to False.\n        \"\"\"\n        if not hasattr(self, 'results'):\n            self.search()\n\n        active_results = self.active_results\n\n        if not active_results:\n            print(f\"{Fore.YELLOW}No results to summarize.\")\n            return\n\n        ascending_stacks = {}\n        descending_stacks = {}\n\n        for key, items in active_results.items():\n            if not items: continue\n            direction = items[0].properties.get('flightDirection', 'UNKNOWN').upper()\n\n            if direction == 'ASCENDING':\n                ascending_stacks[key] = items\n            elif direction == 'DESCENDING':\n                descending_stacks[key] = items\n\n        def _print_group(label, data_dict, color_code):\n            if not data_dict:\n                return\n            print(f\"\\n{color_code}=== {label} ORBITS ({len(data_dict)} Stacks) ==={Fore.RESET}\")\n            sorted_keys = sorted(data_dict.keys())\n\n            for key in sorted_keys:\n                    items = data_dict[key]\n                    count = len(items)\n\n                    # Calculate time range\n                    dates = [isoparse(i.properties['startTime']) for i in items]\n                    start_date = min(dates).date()\n                    end_date = max(dates).date()\n\n                    print(f\"Path {key[0]} Frame {key[1]} | Count: {count} | {start_date} --&gt; {end_date}\")\n\n                    if ls:\n                        # Sort scenes by date\n                        items_sorted = sorted(items, key=lambda x: isoparse(x.properties['startTime']))\n                        for scene in items_sorted:\n                            scene_date = isoparse(scene.properties['startTime']).date()\n                            print(f\"    {Fore.LIGHTBLACK_EX}{scene.properties['sceneName']} ({scene_date}){Fore.RESET}\")\n        if ascending_stacks:\n            _print_group(\"ASCENDING\", ascending_stacks, Fore.MAGENTA)\n\n        if descending_stacks:\n            _print_group(\"DESCENDING\", descending_stacks, Fore.CYAN)\n\n        print(\"\") # Final newline\n\n\n    def footprint(self, save_path: str | None = None):\n        \"\"\"Display or save the search result footprints and AOI using matplotlib.\n\n        Args:\n            save_path (str, optional): Path to save the figure. If None, displays interactively.\n                Defaults to None.\n        \"\"\"\n        results_to_plot = self.active_results\n        if not results_to_plot:\n            print(f\"{Fore.RED}No results to plot.\")\n            return\n\n        transformer = Transformer.from_crs(\"EPSG:4326\", \"EPSG:3857\", always_xy=True)\n        N = len(results_to_plot)\n        cmap = plt.cm.get_cmap('hsv', N+1)\n\n        fig, ax = plt.subplots(1, 1, figsize=(10,10), dpi=150)\n\n        geom_aoi = transform(transformer.transform, wkt.loads(self.config.intersectsWith))\n        global_minx, global_miny, global_maxx, global_maxy = geom_aoi.bounds\n        plotting.plot_polygon(geom_aoi, ax=ax, edgecolor='red', facecolor='none', linewidth=2, linestyle='--')\n\n        label_x_aoi = global_maxx - 0.01 * (global_maxx - global_minx)\n        label_y_aoi = global_maxy - 0.01 * (global_maxy - global_miny)\n        plt.text(label_x_aoi, label_y_aoi,\n             f\"AOI\",\n             horizontalalignment='right', verticalalignment='top',\n             fontsize=12, color='red', fontweight='bold',\n             bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', boxstyle='round,pad=0.3'))\n\n        for i, (key, results) in enumerate(results_to_plot.items()):\n            geom = transform(transformer.transform, shape(results[0].geometry))\n            minx, miny, maxx, maxy = geom.bounds\n\n            global_minx = min(global_minx, minx)\n            global_miny = min(global_miny, miny)\n            global_maxx = max(global_maxx, maxx)\n            global_maxy = max(global_maxy, maxy)\n\n            label_x = maxx - 0.01 * (maxx - minx)\n            label_y = maxy - 0.01 * (maxy - miny)\n\n            plt.text(label_x, label_y,\n             f\"Path: {key[0]}\\nFrame: {key[1]}\\nStack: {len(results)}\",\n             horizontalalignment='right', verticalalignment='top',\n             fontsize=12, color=cmap(i), fontweight='bold',\n             bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', boxstyle='round,pad=0.3'))\n\n            for result in results:\n                geom = transform(transformer.transform, shape(result.geometry))\n                x, y = geom.exterior.xy\n                ax.plot(x, y, color=cmap(i))\n\n        ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)\n\n        ax.set_xlim(global_minx, global_maxx)\n        ax.set_ylim(global_miny, global_maxy)\n\n        ax.set_axis_off()\n        if save_path is not None:\n            save_path = Path(save_path).expanduser().resolve()\n            plt.savefig(save_path.as_posix(), dpi=300, bbox_inches='tight')\n            print(f\"Footprint figure saved to {save_path}\")\n        else:\n            plt.subplots_adjust(top = 1, bottom = 0, right = 1, left = 0, hspace = 0, wspace = 0)\n            plt.show()\n\n    def filter(self, \n                path_frame : tuple | list[tuple] | None = None,\n                start: str | None = None,\n                end: str | None = None,\n                frame: int | list[int] | None = None, \n                asfFrame: int | list[int] | None = None, \n                flightDirection: str | None = None,\n                relativeOrbit: int | list[int] | None = None,\n                absoluteOrbit: int | list[int] | None = None,\n                lookDirection: str | None = None,\n                polarization: str | list[str] | None = None,\n                processingLevel: str | None = None,\n                beamMode: str | None = None,\n                season: list[int] | None = None,\n                min_coverage: float | None = None,\n                min_count: int | None = None,\n                max_count: int | None = None,\n                latest_n: int | None = None,\n                earliest_n: int | None = None\n               ) -&gt; dict:\n        \"\"\"Filter active results by various properties after search.\n\n        Args:\n            path_frame (tuple | list[tuple], optional): A single (path, frame) tuple or list of tuples.\n                Defaults to None.\n            start (str, optional): Start date string, e.g. '2021-01-01'. Defaults to None.\n            end (str, optional): End date string, e.g. '2023-12-31'. Defaults to None.\n            frame (int | list[int], optional): Sensor native frame number(s), e.g. 50. Defaults to None.\n            asfFrame (int | list[int], optional): ASF internal frame number(s), e.g. 50. Defaults to None.\n            flightDirection (str, optional): 'ASCENDING' or 'DESCENDING'. Defaults to None.\n            relativeOrbit (int | list[int], optional): Relative orbit number(s) to keep. Defaults to None.\n            absoluteOrbit (int | list[int], optional): Absolute orbit number(s) to keep. Defaults to None.\n            lookDirection (str, optional): 'LEFT' or 'RIGHT'. Defaults to None.\n            polarization (str | list[str], optional): Polarization(s) to keep, e.g. 'VV' or ['VV', 'VH']. \n                Defaults to None.\n            processingLevel (str, optional): Processing level to keep, e.g. 'SLC'. Defaults to None.\n            beamMode (str, optional): Beam mode to keep, e.g. 'IW'. Defaults to None.\n            season (list[int], optional): List of months (1-12) to keep, e.g. [6, 7, 8] for summer. \n                Defaults to None.\n            min_coverage (float, optional): Minimum fractional overlap (0-1) between scene and AOI. \n                Defaults to None.\n            min_count (int, optional): Drop stacks with fewer than this many scenes after filtering. \n                Defaults to None.\n            max_count (int, optional): Keep at most this many scenes per stack (from earliest). \n                Defaults to None.\n            latest_n (int, optional): Keep the N most recent scenes per stack. Defaults to None.\n            earliest_n (int, optional): Keep the N earliest scenes per stack. Defaults to None.\n\n        Returns:\n            dict: Filtered results grouped by (path, frame).\n\n        Raises:\n            ValueError: If no search results are available.\n        \"\"\"\n\n        if not hasattr(self, 'results'):\n            raise ValueError(f\"{Fore.RED}No search results found. Please run search() first.\")\n\n        source = self.active_results\n        filtered = defaultdict(list)\n        prop_keys = self._get_property_keys()\n\n        # --- Pre-process filter values ---\n        if path_frame is not None:\n            targets = {path_frame} if isinstance(path_frame, tuple) else set(path_frame)\n        else:\n            targets = None\n\n        start_dt = isoparse(start).replace(tzinfo=None) if start else None\n        end_dt   = isoparse(end).replace(tzinfo=None)   if end   else None\n        frames     = {frame}    if isinstance(frame, int)    else set(frame)    if frame    else None\n        asf_frames = {asfFrame} if isinstance(asfFrame, int) else set(asfFrame) if asfFrame else None\n        relative_orbits  = {relativeOrbit}  if isinstance(relativeOrbit, int)  else set(relativeOrbit)  if relativeOrbit  else None\n        absolute_orbits  = {absoluteOrbit}  if isinstance(absoluteOrbit, int)  else set(absoluteOrbit)  if absoluteOrbit  else None\n        polarizations    = {polarization}   if isinstance(polarization, str)   else set(polarization)   if polarization   else None\n        season_months    = set(season) if season else None\n\n        if min_coverage is not None:\n            aoi_geom = wkt.loads(self.config.intersectsWith)\n\n        for key, items in source.items():\n            if targets is not None and key not in targets:\n                continue\n\n            if flightDirection:\n                stack_dir = items[0].properties.get('flightDirection', '').upper()\n                if stack_dir != flightDirection.upper():\n                    continue\n\n            if lookDirection:\n                stack_look = items[0].properties.get('lookDirection', '').upper()\n                if stack_look != lookDirection.upper():\n                    continue\n\n            if beamMode:\n                stack_beam = items[0].properties.get('beamMode', '').upper()\n                if stack_beam != beamMode.upper():\n                    continue\n\n            if processingLevel:\n                stack_proc = items[0].properties.get('processingLevel', '').upper()\n                if stack_proc != processingLevel.upper():\n                    continue\n        # --- Scene-level filters ---\n            filtered_items = []\n            for item in items:\n                props = item.properties\n\n                scene_dt = isoparse(props['startTime']).replace(tzinfo=None)\n                # Date range\n                if start_dt and scene_dt &lt; start_dt:\n                    continue\n                if end_dt and scene_dt &gt; end_dt:\n                    continue\n\n                # Native frame filter\n                if frames is not None:\n                    if props.get('frameNumber') not in frames:\n                        continue\n\n                # ASF frame filter\n                if asf_frames is not None:\n                    if props.get('asfFrame') not in asf_frames:\n                        continue\n                # Season (month filter)\n                if season_months and scene_dt.month not in season_months:\n                    continue\n\n                # Relative orbit\n                if relative_orbits and props.get(prop_keys['relativeOrbit']) not in relative_orbits:\n                    continue\n\n                # Absolute orbit\n                if absolute_orbits and props.get(prop_keys['absoluteOrbit']) not in absolute_orbits:\n                    continue\n\n                # Polarization \u2014 props value may be a string like 'VV+VH'\n                if polarizations:\n                    scene_pols = set(props.get(prop_keys['polarization'], '').replace('+', ' ').split())\n                    if not polarizations.intersection(scene_pols):\n                        continue\n\n                if min_coverage is not None:\n                    scene_geom = shape(item.geometry)\n                    intersection = aoi_geom.intersection(scene_geom)\n                    coverage = intersection.area / aoi_geom.area\n                    if coverage &lt; min_coverage:\n                        continue\n\n                filtered_items.append(item)\n            if not filtered_items:\n                continue\n\n\n            filtered_items = sorted(filtered_items, key=lambda x: isoparse(x.properties['startTime']))\n\n            if earliest_n is not None:\n                filtered_items = filtered_items[:earliest_n]\n            elif latest_n is not None:\n                filtered_items = filtered_items[-latest_n:]\n            elif max_count is not None:\n                filtered_items = filtered_items[:max_count]\n\n            if min_count is not None and len(filtered_items) &lt; min_count:\n                print(f\"{Fore.YELLOW}Stack Path {key[0]} Frame {key[1]} dropped: only {len(filtered_items)} scenes (min_count={min_count}).\")\n                continue\n\n            filtered[key] = filtered_items\n\n        if not filtered:\n            print(f\"{Fore.YELLOW}Warning: No results matched the given filters.\")\n        else:\n            self._subset = filtered\n            total_scenes = sum(len(v) for v in filtered.values())\n            print(f\"{Fore.GREEN}Filter applied. {len(filtered)} stacks, {total_scenes} total scenes remaining.\")\n\n        return filtered\n\n    def dem(self, save_path: str | None = None):\n        \"\"\"Download DEM for co-registration uses.\n\n        Args:\n            save_path (str, optional): Directory to save DEM files. If None, uses config.workdir.\n                Defaults to None.\n\n        Returns:\n            tuple: (X, p) where X is the DEM array and p is the rasterio profile.\n        \"\"\"\n        output_dir = Path(save_path).expanduser().resolve() if save_path else self.config.workdir\n\n        for key, results in self.active_results.items():\n            download_path = output_dir.joinpath(f'dem',f'p{key[0]}_f{key[1]}')\n            download_path.mkdir(exist_ok=True, parents=True)\n            geom = shape(results[0].geometry)\n            west_lon, south_lat, east_lon, north_lat =  geom.bounds\n            bbox = [ west_lon, south_lat, east_lon, north_lat]\n            X, p = dem_stitcher.stitch_dem(\n                bbox, \n                dem_name='glo_30',\n                dst_area_or_point='Point',\n                dst_ellipsoidal_height=True\n            )\n\n            with rio.open(download_path.joinpath(f'dem_p{key[0]}_f{key[1]}.tif'), 'w', **p) as ds:\n                    ds.write(X,1)\n                    ds.update_tags(AREA_OR_POINT='Point')\n        return X, p\n\n    def download(self, save_path: str | None = None, max_workers: int = 3):\n        \"\"\"Download the search results to the specified output directory.\n\n        Args:\n            save_path (str, optional): Download path. If None, uses config.workdir. \n                Defaults to None.\n            max_workers (int, optional): Number of concurrent downloads. 3-5 recommended \n                for ASF. Set to 1 to disable multithreading. Defaults to 3.\n\n        Raises:\n            ValueError: If no search results are available.\n        \"\"\"\n        from concurrent.futures import ThreadPoolExecutor, as_completed\n        from threading import Event\n        output_dir = Path(save_path).expanduser().resolve() if save_path else self.config.workdir\n\n        self.download_dir = output_dir.joinpath('data')\n        self.download_dir.mkdir(exist_ok=True, parents=True)\n\n        if not hasattr(self, 'results'):\n            raise ValueError(f\"{Fore.RED}No search results found. Please run search() first.\")\n\n        stop_event = Event()\n\n        jobs = []\n        for key, results in self.active_results.items():\n            download_path = self.download_dir.joinpath(f'p{key[0]}_f{key[1]}')\n            download_path.mkdir(parents=True, exist_ok=True)\n            for result in results:\n                jobs.append((key, result, download_path))\n\n        total_jobs   = len(jobs)\n        success_count = 0\n        failure_count = 0\n        failed_files  = []\n\n        active_files: dict[int, Path] = {}\n        active_files_lock = __import__('threading').Lock()\n\n        print(f\"Downloading {total_jobs} scenes across \"\n          f\"{len(self.active_results)} stacks \"\n          f\"({max_workers} concurrent)...\\n\")\n\n        def _stream_download_interruptible(url, file_path, expected_bytes, \n                                        pbar_position, scene_name):\n            \"\"\"Stream download that checks stop_event on every chunk.\"\"\"\n            from tqdm import tqdm\n            from asf_search.download.download import _try_get_response\n\n            thread_session = asf.ASFSession()\n            thread_session.cookies.update(self.session.cookies)\n            thread_session.headers.update(self.session.headers)\n\n            for attempt in range(1, 4):\n                if stop_event.is_set():\n                    raise InterruptedError(\"Download cancelled by user.\")\n                try:\n                    response = _try_get_response(session=thread_session, url=url)\n                    total_bytes = int(response.headers.get('content-length', expected_bytes))\n\n                    with tqdm(\n                        total=total_bytes,\n                        unit='B',\n                        unit_scale=True,\n                        unit_divisor=1024,\n                        desc=f\"[Worker {pbar_position+1}] {scene_name}\",\n                        bar_format='{desc:&lt;60}{percentage:3.0f}%|{bar:25}{r_bar}',\n                        colour='green',\n                        position=pbar_position,\n                        leave=True,\n                    ) as pbar:\n                        with open(file_path, 'wb') as f:\n                            for chunk in response.iter_content(chunk_size=65536):\n                                # Check stop event on EVERY chunk \u2014 this is the key\n                                if stop_event.is_set():\n                                    response.close()  # abort the connection immediately\n                                    raise InterruptedError(\"Download cancelled by user.\")\n                                if chunk:\n                                    f.write(chunk)\n                                    pbar.update(len(chunk))\n                    return  # success\n\n                except InterruptedError:\n                    raise  # propagate immediately, don't retry\n                except Exception as e:\n                    if file_path.exists():\n                        file_path.unlink()\n                    if attempt == 3:\n                        raise\n                    time.sleep(2 ** attempt)\n\n        def _download_job(args):\n            key, result, download_path, position = args\n            file_id   = result.properties['fileID']\n            size_b    = result.properties['bytes']\n            size_mb   = size_b / (1024 * 1024)\n            filename  = result.properties.get('fileName', f\"{file_id}.zip\")\n            file_path = download_path / filename\n\n            scene_name = result.properties.get('sceneName', file_id)\n\n            if stop_event.is_set():\n                return file_id, 'cancelled', 0, None\n\n            # Skip if already complete\n            if file_path.exists() and file_path.stat().st_size == size_b:\n                return file_id, 'skipped', size_mb, None\n\n            # Remove incomplete file\n            if file_path.exists():\n                file_path.unlink()\n\n            with active_files_lock:\n                active_files[position] = file_path\n\n            try:\n                start_time = time.time()\n                _stream_download_interruptible(\n                    url=result.properties['url'],\n                    file_path=file_path,\n                    expected_bytes=size_b,\n                    pbar_position=position,\n                    scene_name=scene_name,\n                )\n\n                actual_size = file_path.stat().st_size\n                if actual_size != size_b:\n                    raise IOError(f\"Size mismatch: expected {size_b}, got {actual_size} bytes.\")\n\n                elapsed = time.time() - start_time\n                speed   = size_mb / elapsed if elapsed &gt; 0 else 0\n                return file_id, 'success', speed, None\n\n            except InterruptedError:\n                return file_id, 'cancelled', 0, None\n\n            except Exception as e:\n                if file_path.exists():\n                    file_path.unlink()\n                return file_id, 'failed', 0, str(e)\n            finally:\n                with active_files_lock:\n                    active_files.pop(position, None)\n        job_args = [\n            (key, result, download_path, i % max_workers) \n            for i, (key, result, download_path) in enumerate(jobs)\n        ]\n\n        executor = ThreadPoolExecutor(max_workers=max_workers)\n        futures  = {executor.submit(_download_job, args): args for args in job_args}\n\n        try:\n            for future in as_completed(futures):\n                file_id, status, value, error, _ = future.result()\n\n                if status == 'success':\n                    print(f\"  {Fore.GREEN}\u2714 {file_id} ({value:.1f} MB/s)\")\n                    success_count += 1\n                elif status == 'skipped':\n                    print(f\"  {Fore.YELLOW}\u23ed {file_id} ({value:.1f} MB, already exists)\")\n                    success_count += 1\n                elif status == 'cancelled':\n                    pass  # silently skip cancelled jobs\n                else:\n                    print(f\"  {Fore.RED}\u2718 {file_id} \u2014 {error}\")\n                    failure_count += 1\n                    failed_files.append(file_id)\n        except KeyboardInterrupt:\n            print(f\"\\n{Fore.YELLOW}\u26a0 Download interrupted by user. Cancelling pending jobs...\")\n            stop_event.set()\n            # Cancel all pending futures that haven't started yet\n            for future in futures:\n                future.cancel()\n\n            # Shut down without waiting for running threads to finish\n            executor.shutdown(wait=False, cancel_futures=True)\n\n            # Clean up any partial files being actively written\n            with active_files_lock:\n                for position, file_path in active_files.items():\n                    if file_path.exists():\n                        print(f\"  {Fore.RED}Removing partial file: {file_path.name}\")\n                        file_path.unlink()\n\n            print(f\"{Fore.YELLOW}Download cancelled. \"\n                    f\"{success_count} scenes completed before interrupt.\")\n            return\n\n        else:\n            executor.shutdown(wait=True)\n\n        # Final summary\n        print(\"\\n\" + \"\u2500\" * 60)\n        print(f\"Download complete: {Fore.GREEN}{success_count}/{total_jobs} succeeded{Fore.RESET}\", end=\"\")\n        if failure_count:\n            print(f\", {Fore.RED}{failure_count}/{total_jobs} failed{Fore.RESET}\")\n            print(f\"\\nFailed files:\")\n            for f in failed_files:\n                print(f\"  {Fore.RED}- {f}\")\n        print(f\"\\nFiles saved to: {self.download_dir}\")\n</code></pre>"},{"location":"quickstart/downloader/#usage","title":"Usage","text":"<ul> <li> <p>Create downloader with parameters</p> <p>Initialize a downloader instance with search criteria <pre><code>s1 = Downloader.create('ASF_base_Downloader', \n                        intersectsWith=[-113.05, 37.74, -112.68, 38.00],\n                        dataset='SENTINEL-1',\n                        instrument='C-SAR',\n                        beamMode='IW',\n                        polarization=['VV', 'VV+VH'],\n                        processingLevel='SLC'\n                        start='2020-01-01', \n                        end='2020-12-31',  \n                        relativeOrbit=100, \n                        frame=466, \n                        workdir='path/to/dir')\n</code></pre> OR: <pre><code>params = {\n    \"intersectsWith\": [-113.05, 37.74, -112.68, 38.00],\n    \"dataset\": \"SENTINEL-1\",\n    \"instrument\": \"C-SAR\",\n    \"beamMode\": \"IW\",\n    \"polarization\": [\"VV\", \"VV+VH\"],\n    \"processingLevel\": \"SLC\",\n    \"start\": \"2020-01-01\",\n    \"end\": \"2020-12-31\",\n    \"relativeOrbit\": 100,\n    \"frame\": 466,\n    \"workdir\": \"path/to/dir\"\n}\ndl = Downloader.create('ASF_base_Downloader', **params)\n</code></pre> OR <pre><code>from insarscript.config import ASF_Base_Config\ncfg = ASF_Base_Config(intersectsWith=[-113.05, 37.74, -112.68, 38.00],\n                        dataset='SENTINEL-1',\n                        instrument='C-SAR',\n                        beamMode='IW',\n                        polarization=['VV', 'VV+VH'],\n                        processingLevel='SLC'\n                        start='2020-01-01', \n                        end='2020-12-31',  \n                        relativeOrbit=100, \n                        frame=466, \n                        workdir='path/to/dir')\ndl = Downloader.create('ASF_base_Downloader', config=cfg)\n</code></pre></p> <p>The base configure <code>ASF_Base_Config</code> contains all parameters from asf_search keywords. For detailed descriptions and usage of each parameter, please refer to the official ASF Search documentation.</p> Source code in <code>src/insarscript/config/defaultconfig.py</code> <pre><code>@dataclass\nclass ASF_Base_Config:\n    '''\n    Dataclass containing all configuration options for asf_search.\n\n    This class provides a unified interface for configuring ASF (Alaska Satellite Facility) \n    search parameters.\n    '''\n    name: str = \"ASF_Base_Config\"\n    dataset: str | list[str] | None = None\n    platform: str | list[str] | None = None\n    instrument: str | None = None\n    absoluteBurstID: int | list[int] | None = None\n    absoluteOrbit: int | list[int] | None = None\n    asfFrame: int | list[int] | None = None\n    beamMode: str | None = None\n    beamSwath: str | list[str] | None = None\n    campaign: str | None = None\n    maxDoppler: float | None = None\n    minDoppler: float | None = None\n    maxFaradayRotation: float | None = None\n    minFaradayRotation: float | None = None\n    flightDirection: str | None = None\n    flightLine: str | None = None\n    frame: int | list[int] | None = None\n    frameCoverage: str | None = None\n    fullBurstID: str | list[str] | None = None\n    groupID: str | None = None\n    jointObservation: bool | None = None\n    lookDirection: str | None = None\n    offNadirAngle: float | list[float] | None = None\n    operaBurstID: str | list[str] | None = None\n    polarization: str | list[str] | None = None\n    mainBandPolarization: str | list[str] | None = None\n    sideBandPolarization: str | list[str] | None = None\n    processingLevel: str | None = None\n    productionConfiguration: str | list[str] | None = None\n    rangeBandwidth: str | list[str] | None = None\n    relativeBurstID: str | list[str] | None = None\n    relativeOrbit: int | list[int] | None = None\n    intersectsWith: str | None = None  \n    processingDate: str | None = None\n    start: str | None = None\n    end: str | None = None\n    season: list[int] | None = None\n    stack_from_id: str | None = None\n    maxResults: int | None = None\n    workdir: Path | str = field(default_factory=lambda: Path.cwd()) \n\n    def __post_init__(self):\n        if isinstance(self.workdir, str):\n            self.workdir = Path(self.workdir).expanduser().resolve()\n</code></pre> </li> <li> <p>Search</p> <p>Query the satellite archive and retrieve available scenes matching your criteria <pre><code>results = dl.search()\n</code></pre></p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If search returns no results.</p> <code>Exception</code> <p>If search fails after 10 retry attempts.</p> </li> <li> <p>Filter</p> <p>Refine existing search results by applying additional constraints <pre><code>filter_result = dl.filter(start='2020-02-01')\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>path_frame</code> <code>tuple | list[tuple]</code> <p>A single (path, frame) tuple or list of tuples. Defaults to None.</p> <code>None</code> <code>start</code> <code>str</code> <p>Start date string, e.g. '2021-01-01'. Defaults to None.</p> <code>None</code> <code>end</code> <code>str</code> <p>End date string, e.g. '2023-12-31'. Defaults to None.</p> <code>None</code> <code>frame</code> <code>int | list[int]</code> <p>Sensor native frame number(s), e.g. 50. Defaults to None.</p> <code>None</code> <code>asfFrame</code> <code>int | list[int]</code> <p>ASF internal frame number(s), e.g. 50. Defaults to None.</p> <code>None</code> <code>flightDirection</code> <code>str</code> <p>'ASCENDING' or 'DESCENDING'. Defaults to None.</p> <code>None</code> <code>relativeOrbit</code> <code>int | list[int]</code> <p>Relative orbit number(s) to keep. Defaults to None.</p> <code>None</code> <code>absoluteOrbit</code> <code>int | list[int]</code> <p>Absolute orbit number(s) to keep. Defaults to None.</p> <code>None</code> <code>lookDirection</code> <code>str</code> <p>'LEFT' or 'RIGHT'. Defaults to None.</p> <code>None</code> <code>polarization</code> <code>str | list[str]</code> <p>Polarization(s) to keep, e.g. 'VV' or ['VV', 'VH'].  Defaults to None.</p> <code>None</code> <code>processingLevel</code> <code>str</code> <p>Processing level to keep, e.g. 'SLC'. Defaults to None.</p> <code>None</code> <code>beamMode</code> <code>str</code> <p>Beam mode to keep, e.g. 'IW'. Defaults to None.</p> <code>None</code> <code>season</code> <code>list[int]</code> <p>List of months (1-12) to keep, e.g. [6, 7, 8] for summer.  Defaults to None.</p> <code>None</code> <code>min_coverage</code> <code>float</code> <p>Minimum fractional overlap (0-1) between scene and AOI.  Defaults to None.</p> <code>None</code> <code>min_count</code> <code>int</code> <p>Drop stacks with fewer than this many scenes after filtering.  Defaults to None.</p> <code>None</code> <code>max_count</code> <code>int</code> <p>Keep at most this many scenes per stack (from earliest).  Defaults to None.</p> <code>None</code> <code>latest_n</code> <code>int</code> <p>Keep the N most recent scenes per stack. Defaults to None.</p> <code>None</code> <code>earliest_n</code> <code>int</code> <p>Keep the N earliest scenes per stack. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no search results are available.</p> </li> <li> <p>Reset filter</p> <p>Restore search results to the original unfiltered state <pre><code>dl.reset()\n</code></pre> </p> </li> <li> <p>Summary</p> <p>Display statistics and overview of current search results <pre><code>dl.summary()\n</code></pre> </p> <p>Parameters:</p> Name Type Description Default <code>ls</code> <code>bool</code> <p>If True, list individual scene names and dates.  Defaults to False.</p> <code>False</code> </li> <li> <p>View Footprint</p> <p>Visualize geographic coverage of search results on an interactive map <pre><code>dl.footprint()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>save_path</code> <code>str</code> <p>Path to save the figure. If None, displays interactively. Defaults to None.</p> <code>None</code> </li> <li> <p>Download</p> <p>Download all scenes from current search results to local storage <pre><code>dl.download()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>save_path</code> <code>str</code> <p>Download path. If None, uses config.workdir.  Defaults to None.</p> <code>None</code> <code>max_workers</code> <code>int</code> <p>Number of concurrent downloads. 3-5 recommended  for ASF. Set to 1 to disable multithreading. Defaults to 3.</p> <code>3</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no search results are available.</p> </li> <li> <p>DEM Download</p> <p>Download DEM that covers all scenes from current search results to local storatge <pre><code>dl.dem()\n</code></pre> </p> <p>Parameters:</p> Name Type Description Default <code>save_path</code> <code>str</code> <p>Directory to save DEM files. If None, uses config.workdir. Defaults to None.</p> <code>None</code> </li> </ul>"},{"location":"quickstart/downloader/#s1_slc","title":"S1_SLC","text":"<p>S1_SLC is a specialized downloader that extends ASF_Base_Downloader, preconfigured specifically for downloading Sentinel-1 SLC data.</p> Source code in <code>src/insarscript/downloader/s1_slc.py</code> <pre><code>class S1_SLC(ASF_Base_Downloader):\n    name = \"S1_SLC\"\n    default_config = S1_SLC_Config\n\n    \"\"\"\n    A class to search and download Sentinel-1 data using ASF Search API.\"\"\"\n\n    def download(self, save_path: str | None = None, max_workers: int= 3, force_asf: bool = False, download_orbit: bool = False):\n        \"\"\"Download SLC data and optionally associated orbit files.\n\n        This method downloads the primary SLC data using the base downloader functionality.\n        If `download_orbit` is True, it will also attempt to download orbit files from either\n        ASF or the Copernicus Data Space Ecosystem (CDSE). Users may be prompted to provide\n        CDSE credentials if not already configured in a `.netrc` file.\n\n        Args:\n            save_path (str | None): Optional path to save the downloaded files. Defaults to None.\n            force_asf (bool): If True, forces downloading orbit files from ASF instead of CDSE. Defaults to False.\n            download_orbit (bool): If True, attempts to download orbit files. Defaults to False.\n\n        Raises:\n            ValueError: If CDSE authentication fails and the user cannot provide valid credentials.\n        \"\"\"\n        super().download(save_path=save_path)\n        if download_orbit:\n            print(f\"\"\"\nOrbit files can be downloaded from both ASF and Copernicus Data Space Ecosystem (CDSE) servers. Generally CDSE release orbit files a few hours to days earlier.\nTo download orbit file from Copernicus Data Space Ecosystem(CDSE). Please ensure you to create an account at https://dataspace.copernicus.eu/ and setup in the .netrc file.\nIf a .netrc file is not provide under your home directory, you will be prompt to enter your CDSE username and password. \nCheck documentation for how to setup .netrc file.\\n\nIF you wish to download oribit files from ASF and skip CDSE, use .download(forceasf=True).\"\"\")\n\n            self._has_cdse_netrc = self._check_netrc(keyword='machine dataspace.copernicus.eu')\n            if self._has_cdse_netrc:\n                print(f\"{Fore.GREEN}Credential from .netrc was found for authentication.\\n\")\n            else: \n                while True:\n                    self._cdse_username = input(\"Enter your CDSE username: \")\n                    self._cdse_password = getpass.getpass(\"Enter your CDSE password: \")\n                    if not self._check_cdse_credentials(self._cdse_username, self._cdse_password): \n                        print(f\"{Fore.RED}Authentication failed. Please check your credentials and try again.\\n\")\n                        continue\n                    else:\n                        print(f\"{Fore.GREEN}Authentication successful.\\n\")\n                        netrc_path = Path.home().joinpath(\".netrc\")\n                        cdse_entry = f\"\\nmachine dataspace.copernicus.eu\\n    login {self._cdse_username}\\n    password {self._cdse_password}\\n\"\n                        with open(netrc_path, 'a') as f:\n                            f.write(cdse_entry)\n                        print(f\"{Fore.GREEN}Credentials saved to {netrc_path}. You can now download orbit from CDSE without entering credentials again.\\n\")\n                        break\n            print(f\"Downloading orbit files for SLCs...\")\n            for key, results in self.results.items():\n                download_path = self.download_dir.joinpath(f'p{key[0]}_f{key[1]}')\n                for i, result in enumerate(results, start=1):\n                    print(f\"Searching orbit files for {i}/{len(results)}: {result.properties['fileID']}\")\n                    scene_name = result.properties['sceneName']\n                    print(f\"Searching orbit for {scene_name}\")\n                    scene_info = result.properties['sceneName'].replace(\"__\", \"_\").split(\"_\")\n                    info = download_eofs(\n                        orbit_dts=[scene_name.replace(\"__\", \"_\").split(\"_\")[4]],\n                        missions=[scene_name.split(\"_\")[0]],\n                        save_dir=download_path.as_posix(),\n                        force_asf=force_asf\n                    )\n                    if len(info) &gt; 0:\n                        print(f\"{Fore.GREEN}Orbit files for {result.properties['sceneName']}downloaded successfully.\")\n                    else:\n                        print(f\"{Fore.YELLOW}No orbit files found for the given parameters.\")\n\n    def _check_cdse_credentials(self, username: str, password: str) -&gt; bool:\n        url = \"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\"\n        data = {\n            \"grant_type\": \"password\",\n            \"client_id\": \"cdse-public\",\n            \"username\": username,\n            \"password\": password\n        }\n        resp = requests.post(url, data=data)\n        return resp.status_code == 200 and \"access_token\" in resp.json()\n</code></pre>"},{"location":"quickstart/downloader/#usage_1","title":"Usage","text":"<ul> <li> <p>Create downloader with parameters</p> <p>Initialize a downloader instance with search criteria <pre><code>s1 = Downloader.create('S1_SLC', \n                        intersectsWith=[-113.05, 37.74, -112.68, 38.00],\n                        start='2020-01-01', \n                        end='2020-12-31',  \n                        relativeOrbit=100, \n                        frame=466, \n                        workdir='path/to/dir')\n</code></pre> OR <pre><code>params = {\n    \"intersectsWith\": [-113.05, 37.74, -112.68, 38.00],\n    \"start\": \"2020-01-01\",\n    \"end\": \"2020-12-31\",\n    \"relativeOrbit\": 100,\n    \"frame\": 466,\n    \"workdir\": \"path/to/dir\"\n}\ndl = Downloader.create('S1_SLC', **params)\n</code></pre> OR <pre><code>from insarscript.config import S1_SLC_Config\ncfg = S1_SLC_Config(intersectsWith= [-113.05, 37.74, -112.68, 38.00],\n                    start= \"2020-01-01\",\n                    end= \"2020-12-31\",\n                    relativeOrbit= 100,\n                    frame= 466,\n                    workdir= \"path/to/dir\")\ndl = Downloader.create('S1_SLC', config=cfg)\n</code></pre></p> <p>The configure <code>S1_SLC_config</code> contains pre-defined parameters specifically for Sentinel-1 data. For detailed descriptions and usage of each parameter, please refer to the official ASF Search documentation.</p> Source code in <code>src/insarscript/config/defaultconfig.py</code> <pre><code>@dataclass\nclass S1_SLC_Config(ASF_Base_Config):\n    name:str = \"S1_SLC_Config\"\n    dataset: str | list[str] | None =  constants.DATASET.SENTINEL1\n    instrument: str | None = constants.INSTRUMENT.C_SAR\n    beamMode:str | None = constants.BEAMMODE.IW\n    polarization: str|list[str] | None = field(default_factory=lambda: [constants.POLARIZATION.VV, constants.POLARIZATION.VV_VH])\n    processingLevel: str | None = constants.PRODUCT_TYPE.SLC\n</code></pre> </li> <li> <p>Search</p> <p>Query the satellite archive and retrieve available scenes matching your criteria <pre><code>results = dl.search()\n</code></pre></p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If search returns no results.</p> <code>Exception</code> <p>If search fails after 10 retry attempts.</p> </li> <li> <p>Filter</p> <p>Refine existing search results by applying additional constraints <pre><code>filter_result = dl.filter(start='2020-02-01')\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>path_frame</code> <code>tuple | list[tuple]</code> <p>A single (path, frame) tuple or list of tuples. Defaults to None.</p> <code>None</code> <code>start</code> <code>str</code> <p>Start date string, e.g. '2021-01-01'. Defaults to None.</p> <code>None</code> <code>end</code> <code>str</code> <p>End date string, e.g. '2023-12-31'. Defaults to None.</p> <code>None</code> <code>frame</code> <code>int | list[int]</code> <p>Sensor native frame number(s), e.g. 50. Defaults to None.</p> <code>None</code> <code>asfFrame</code> <code>int | list[int]</code> <p>ASF internal frame number(s), e.g. 50. Defaults to None.</p> <code>None</code> <code>flightDirection</code> <code>str</code> <p>'ASCENDING' or 'DESCENDING'. Defaults to None.</p> <code>None</code> <code>relativeOrbit</code> <code>int | list[int]</code> <p>Relative orbit number(s) to keep. Defaults to None.</p> <code>None</code> <code>absoluteOrbit</code> <code>int | list[int]</code> <p>Absolute orbit number(s) to keep. Defaults to None.</p> <code>None</code> <code>lookDirection</code> <code>str</code> <p>'LEFT' or 'RIGHT'. Defaults to None.</p> <code>None</code> <code>polarization</code> <code>str | list[str]</code> <p>Polarization(s) to keep, e.g. 'VV' or ['VV', 'VH'].  Defaults to None.</p> <code>None</code> <code>processingLevel</code> <code>str</code> <p>Processing level to keep, e.g. 'SLC'. Defaults to None.</p> <code>None</code> <code>beamMode</code> <code>str</code> <p>Beam mode to keep, e.g. 'IW'. Defaults to None.</p> <code>None</code> <code>season</code> <code>list[int]</code> <p>List of months (1-12) to keep, e.g. [6, 7, 8] for summer.  Defaults to None.</p> <code>None</code> <code>min_coverage</code> <code>float</code> <p>Minimum fractional overlap (0-1) between scene and AOI.  Defaults to None.</p> <code>None</code> <code>min_count</code> <code>int</code> <p>Drop stacks with fewer than this many scenes after filtering.  Defaults to None.</p> <code>None</code> <code>max_count</code> <code>int</code> <p>Keep at most this many scenes per stack (from earliest).  Defaults to None.</p> <code>None</code> <code>latest_n</code> <code>int</code> <p>Keep the N most recent scenes per stack. Defaults to None.</p> <code>None</code> <code>earliest_n</code> <code>int</code> <p>Keep the N earliest scenes per stack. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no search results are available.</p> </li> <li> <p>Reset filter</p> <p>Restore search results to the original unfiltered state <pre><code>dl.reset()\n</code></pre> </p> </li> <li> <p>Summary</p> <p>Display statistics and overview of current search results <pre><code>dl.summary()\n</code></pre> </p> <p>Parameters:</p> Name Type Description Default <code>ls</code> <code>bool</code> <p>If True, list individual scene names and dates.  Defaults to False.</p> <code>False</code> </li> <li> <p>View Footprint</p> <p>Visualize geographic coverage of search results on an interactive map <pre><code>dl.footprint()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>save_path</code> <code>str</code> <p>Path to save the figure. If None, displays interactively. Defaults to None.</p> <code>None</code> </li> <li> <p>Download</p> <p>Download all scenes from current search results to local storage <pre><code>dl.download()\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>save_path</code> <code>str | None</code> <p>Optional path to save the downloaded files. Defaults to None.</p> <code>None</code> <code>force_asf</code> <code>bool</code> <p>If True, forces downloading orbit files from ASF instead of CDSE. Defaults to False.</p> <code>False</code> <code>download_orbit</code> <code>bool</code> <p>If True, attempts to download orbit files. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If CDSE authentication fails and the user cannot provide valid credentials.</p> </li> <li> <p>DEM Download</p> <p>Download DEM that covers all scenes from current search results to local storatge <pre><code>dl.dem()\n</code></pre> </p> <p>Parameters:</p> Name Type Description Default <code>save_path</code> <code>str</code> <p>Directory to save DEM files. If None, uses config.workdir. Defaults to None.</p> <code>None</code> </li> </ul>"},{"location":"quickstart/install/","title":"Installation","text":""},{"location":"quickstart/install/#installaton","title":"Installaton","text":"<p>You can install the latest available version from Conda:</p> Note <p>You may want to start with a fresh environment by: </p> <p><code>conda create -n insarscript python=3.12 &amp;&amp; conda activate insarscript</code></p> <p><pre><code>conda install insarscript -c conda-forge\n</code></pre> OR </p> <p>from pip:</p> Note <p>Since GDAL depends on non-Python system libraries, we will add it via conda: </p> <pre><code>pip install insarscript\nconda install gdal\n</code></pre>"},{"location":"quickstart/install/#development-setup","title":"Development Setup","text":"<p>InSAR Script is currently under active development, to set up a the latest dev version:</p> <pre><code>git clone https://github.com/jldz9/InSARScript.git\ncd InSARScript\nconda env create -f environment.yml -n insar_dev\nconda activate insar_dev\npip install -e .\n</code></pre> Note <p>Or <code>mamba env create -f environment.yml -n insar_dev</code> </p> <p>if you have mamba installed for faster environment solve</p>"},{"location":"quickstart/processor/","title":"Processor","text":"<p>The InSARScript Processor module provides functionality specifically for interferogram processing.</p> <ul> <li> <p>Import processor</p> <p>Import the Processor class to access all dprocessor functionality <pre><code>from insarscript import Processor\n</code></pre></p> </li> <li> <p>View available processors</p> <p>List all registered processor <pre><code>Processor.available()\n</code></pre></p> </li> </ul>"},{"location":"quickstart/processor/#available-processors","title":"Available Processors","text":""},{"location":"quickstart/processor/#hyp3_insar","title":"Hyp3_InSAR","text":"<p>The HyP3 InSAR processor is a cloud-based processing service provided by the ASF HyP3 system for generating interferograms from Sentinel-1 SAR data. InSARScript wrapped hyp3_sdk as one of its process backends. </p> <p>The <code>Hyp3_InSAR</code> is specificially wrap <code>insar_job</code> in hype_sdk to provide InSAR SLC processing workflows. </p> Source code in <code>src/insarscript/processor/hyp3_insar.py</code> <pre><code>class Hyp3_InSAR(Hyp3Base):\n    name = \"Hyp3_InSAR\"\n    default_config = Hyp3_InSAR_Config\n    def __init__(self, config: Hyp3_InSAR_Config | None = None):\n        super().__init__(config)\n        # Fetch InSAR specific cost table\n        try:\n            self.cost = self.client.costs()['INSAR_GAMMA']['cost_table'][f'{self.config.looks}']\n        except Exception as e:\n            print(f\"{Fore.YELLOW}Warning: Could not fetch InSAR cost table ({e}). Using local cost table.{Fore.RESET}\")\n            if self.config.looks == \"20x4\":\n                self.cost = 10\n            elif self.config.looks == \"10x2\":\n                self.cost = 15\n            else:\n                raise ValueError(f\"{Fore.RED}Unsupported looks configuration: {self.config.looks}. Please provide a valid cost for this looks setting.{Fore.RESET}\")\n\n    def submit(self):\n        \"\"\"\n        Submit InSAR jobs to HyP3 based on the current configuration.\n\n        Prepares job payloads from the `pairs` in the configuration\n        and submits them via `_submit_job_queue`, handling user rotation,\n        batching, and credit checks.\n\n        The job names are automatically generated using the `name_prefix`\n        and scene IDs.\n\n        Raises:\n            ValueError: If `self.config.pairs` is not a tuple of two strings\n                        or a list of tuples of two strings.\n\n        Returns:\n            dict:\n                A dictionary mapping usernames to lists of submitted `Batch` objects.\n\n        Example:\n            ```python\n            processor = Hyp3InSARProcessor(config)\n            batches = processor.submit()\n            for user, batch in batches.items():\n                print(f\"{user} submitted {len(batch)} jobs\")\n            ```\n        \"\"\"\n\n        # Normalize pairs input\n        if isinstance(self.config.pairs, (list, tuple)) and all(isinstance(p, str) for p in self.config.pairs):\n                pairs = [(self.config.pairs[0], self.config.pairs[1])]\n        elif isinstance(self.config.pairs, (list, tuple)) and all(isinstance(p, tuple) for p in self.config.pairs):\n            pairs = self.config.pairs\n        else:\n            raise ValueError(f\"{Fore.RED}Invalid pairs format. Provide a list of tuples or a tuple of two strings.\\n\")\n\n        job_queue: list[dict] = []\n\n        for (ref_id, sec_id) in pairs:\n            # We use the client to help format the dict, but we don't submit yet.\n            # We are preparing the payload for _submit_job_queue\n            job = self.client.prepare_insar_job(\n                granule1=ref_id,\n                granule2=sec_id,\n                name= f\"{self.config.name_prefix}_{ref_id.split('_')[5]}_{sec_id.split('_')[5]}\",\n                include_look_vectors=self.config.include_look_vectors,\n                include_inc_map = self.config.include_inc_map,\n                looks = self.config.looks,\n                include_dem=self.config.include_dem,\n                include_wrapped_phase=self.config.include_wrapped_phase,\n                apply_water_mask=self.config.apply_water_mask,\n                include_displacement_maps=self.config.include_displacement_maps,\n                phase_filter_parameter=self.config.phase_filter_parameter\n            )\n            job_queue.append(job)\n\n        # Send to base class for batching and submission\n        batchs = self._submit_job_queue(job_queue)\n        self.batchs = batchs\n        return batchs\n</code></pre>"},{"location":"quickstart/processor/#usage","title":"Usage","text":"<ul> <li> <p>Create Processor with Parameters</p> <p>Initialize a processor instance with search criteria <pre><code>processor = Processor.create('Hyp3_InSAR', workdir='/your/work/path', pairs=pairs)\n</code></pre> OR <pre><code>params = {\n    \"workdir\":'/your/work/path',\n    \"pairs\":pairs,\n}\nprocessor = Processor.create('Hyp3_InSAR', **params)\n</code></pre> OR <pre><code>from insarscript.config import Hyp3_InSAR_Config\ncfg = Hyp3_InSAR_Config(workdir='/your/work/path', pairs=pairs)\nprocessor = Processor.create('Hyp3_InSAR', config=cfg)\n</code></pre></p> <p>Attributes:</p> Name Type Description <code>workdir</code> <code>Path | str</code> <p>Directory where downloaded products will be stored. If provided as a string, it will be converted to a resolved <code>Path</code> object during initialization.</p> <code>saved_job_path</code> <code>Path | str | None</code> <p>Optional path to a saved job JSON file for reloading previously submitted jobs. If provided as a string, it will be converted to a resolved <code>Path</code> object.</p> <code>earthdata_credentials_pool</code> <code>dict[str, str] | None</code> <p>Dictionary mapping usernames to passwords for managing multiple Earthdata accounts. Used for parallel or quota-aware submissions.</p> <code>skip_existing</code> <code>bool</code> <p>If True, skip submission or download of products that already exist locally.</p> <code>submission_chunk_size</code> <code>int</code> <p>Number of jobs submitted per batch request to the API. Helps avoid request size limits and API throttling.</p> <code>max_workers</code> <code>int</code> <p>Maximum number of worker threads used for concurrent submissions or downloads. Recommended to keep below 8 to avoid overwhelming the API or triggering rate limits.</p> <p>Attributes:</p> Name Type Description <code>pairs</code> <code>list[tuple[str, str]] | None</code> <p>List of Sentinel-1 scene ID pairs in the form [(reference_scene, secondary_scene), ...]. If None, pairs must be provided during submission.</p> <code>name_prefix</code> <code>str | None</code> <p>Prefix added to generated HyP3 job names.</p> <code>include_look_vectors</code> <code>bool</code> <p>If True, include look vector layers in the output product.</p> <code>include_los_displacement</code> <code>bool</code> <p>If True, include line-of-sight (LOS) displacement maps.</p> <code>include_inc_map</code> <code>bool</code> <p>If True, include incidence angle maps.</p> <code>looks</code> <code>str</code> <p>Multi-looking factor in the format \"range x azimuth\" (e.g., \"20x4\").</p> <code>include_dem</code> <code>bool</code> <p>If True, include the DEM used during processing.</p> <code>include_wrapped_phase</code> <code>bool</code> <p>If True, include wrapped interferometric phase output.</p> <code>apply_water_mask</code> <code>bool</code> <p>If True, apply a water mask during processing.</p> <code>include_displacement_maps</code> <code>bool</code> <p>If True, include unwrapped displacement maps.</p> <code>phase_filter_parameter</code> <code>float</code> <p>Phase filtering strength parameter (typically between 0 and 1). Higher values apply stronger filtering.</p> Source code in <code>src/insarscript/config/defaultconfig.py</code> <pre><code>@dataclass\nclass Hyp3_InSAR_Config(Hyp3_Base_Config):\n    \"\"\"\n    Configuration options for `hyp3_sdk` InSAR GAMMA processing jobs.\n\n    This dataclass defines all parameters used when submitting\n    InSAR jobs to the ASF HyP3 service using the GAMMA workflow.\n\n    Attributes:\n        pairs (list[tuple[str, str]] | None):\n            List of Sentinel-1 scene ID pairs in the form\n            [(reference_scene, secondary_scene), ...].\n            If None, pairs must be provided during submission.\n\n        name_prefix (str | None):\n            Prefix added to generated HyP3 job names.\n\n        include_look_vectors (bool):\n            If True, include look vector layers in the output product.\n\n        include_los_displacement (bool):\n            If True, include line-of-sight (LOS) displacement maps.\n\n        include_inc_map (bool):\n            If True, include incidence angle maps.\n\n        looks (str):\n            Multi-looking factor in the format \"range x azimuth\"\n            (e.g., \"20x4\").\n\n        include_dem (bool):\n            If True, include the DEM used during processing.\n\n        include_wrapped_phase (bool):\n            If True, include wrapped interferometric phase output.\n\n        apply_water_mask (bool):\n            If True, apply a water mask during processing.\n\n        include_displacement_maps (bool):\n            If True, include unwrapped displacement maps.\n\n        phase_filter_parameter (float):\n            Phase filtering strength parameter (typically between 0 and 1).\n            Higher values apply stronger filtering.\n    \"\"\"\n\n    name: str = \"Hyp3_InSAR_Config\"\n    pairs: list[tuple[str, str]] | None = None\n    name_prefix: str | None = 'ifg'\n    include_look_vectors:bool=True\n    include_los_displacement:bool=False\n    include_inc_map:bool=True\n    looks:str='20x4'\n    include_dem :bool=True\n    include_wrapped_phase :bool=False\n    apply_water_mask :bool=True\n    include_displacement_maps:bool=True\n    phase_filter_parameter :float=0.6\n</code></pre> </li> <li> <p>Submit Jobs</p> <p>Submit InSAR jobs to HyP3 based on the current configuration.</p> <pre><code>jobs = processor.submit()\n</code></pre> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>self.config.pairs</code> is not a tuple of two strings         or a list of tuples of two strings.</p> </li> <li> <p>Refresh Jobs</p> <p>Refresh the status of all jobs.</p> <pre><code>jobs = processor.refresh()\n</code></pre> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no jobs are loaded in memory.</p> </li> <li> <p>Retry Failed Jobs</p> <p>Retry all failed jobs by re-submitting them.</p> <pre><code>jobs = processor.retry()\n</code></pre> </li> <li> <p>Download Sucessed Jobs</p> <p>Download all succeeded jobs for all users.</p> <pre><code>processor.download()\n</code></pre> </li> <li> <p>Save Current Jobs</p> <p>Save the current job batch information to a JSON file.</p> <pre><code>processor.save()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>save_path</code> <code>Path | str | None</code> <p>Path to save the batch JSON file. If None, defaults to <code>hyp3_jobs.json</code> in <code>self.output_dir</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no job batches exist to save.</p> </li> <li> <p>Watch Jobs</p> <p>Continuously monitor jobs and download completed outputs.</p> <pre><code>processor.watch()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>refresh_interval</code> <code>int</code> <p>Time interval (in seconds) between refreshes.</p> <code>300</code> </li> <li> <p>Load saved job</p> <p>Load previously saved json file and resume the work. </p> <pre><code>processor = Processor.create('Hyp3_InSAR', saved_job_path='path/to/your/json/file.json')\n</code></pre> <p>when load back saved job, you can resume the work to check/download jobs submitted to Hyp3 Server.</p> </li> </ul>"},{"location":"quickstart/running/","title":"Running","text":"<p>This section provides an overview of the complete InSAR time-series processing workflow, guiding you through each stage of the analysis pipeline.</p> <p></p>"},{"location":"quickstart/running/#modules","title":"Modules","text":"<p>The InSAR script is designed with three config-based main modules to cover the entire InSAR processing workflow:</p> <p>Downloader Processor Analyzer</p> <p>You can click on each module to view detailed information later. For now, let's begin by running the program using the basic example.</p>"},{"location":"quickstart/running/#workflow","title":"Workflow","text":"<p>The basic workflow of InSARScript can be brifely described as: </p> <pre><code>graph\n    A[Set AOI] --&gt; B[Searching];\n    B --&gt; C[Result Filtering];\n    C --&gt; D[Intergerogram];\n    D --&gt; F[Time-series Analysis];\n    F --&gt; H[Post-Processing];\n    click A \"#set-aoi\" \"Go to Set AOI section\"\n    click B \"#searching\" \"Go to the Searching section\"\n    click C \"#result-filtering\" \" Go to the Result Filtering section\"\n    click D \"#interferogram\"\n    click F \"#time-series-analysis\"\n    click H \"#post-processing\"\n\n</code></pre>"},{"location":"quickstart/running/#set-aoi","title":"Set AOI","text":"<p>InSARScript allows to define the AOI using bounding box, shapefiles, or WKT:</p>"},{"location":"quickstart/running/#bounding-box","title":"Bounding box","text":"<pre><code>AOI = [-113.05, 37.74, -112.68, 38.00]\n</code></pre> Note <p>The AOI should be specified as [min_long, min_lat, max_long, max_lat] under CRS: EPSG:4326 (WGS84)</p>"},{"location":"quickstart/running/#shapefiles","title":"Shapefiles","text":"<pre><code>AOI = 'path/to/your/shapefile.shp'\n</code></pre>"},{"location":"quickstart/running/#wkt","title":"WKT","text":"<pre><code>AOI = 'POLYGON((-113.05 37.74, -113.05 38.00, -112.68 38.00, -112.68 37.74, -113.05 37.74))'\n</code></pre>"},{"location":"quickstart/running/#searching","title":"Searching","text":"<p>Once the AOI is defined, you can perform searches using the Downloader.</p> <pre><code>from insarscript import Downloader\nAOI = [-113.05, 37.74, -112.68, 38.00]\ns1 = Downloader.create('S1_SLC', intersectsWith=AOI)\nresults = s1.search()\n</code></pre> Output <pre><code>Searching for SLCs....\n-- A total of 991 results found. \n\nThe AOI crosses 18 stacks, you can use .summary() or .footprint() to check footprints and .pick((path_frame)) to specific the stack of scence \nyou would like to download. If use .download() directly will create subfolders under /home/jldz9/dev/InSARScript for each stack\n</code></pre>"},{"location":"quickstart/running/#result-filtering","title":"Result Filtering","text":"<p>Your AOI probably spans multiple scenes. To view the search result footprints, you can use: <pre><code>s1.footprint()\n</code></pre> This will display a footprint map of the available Sentinel-1 scenes that covers the AOI. The stack indicates numbers of SAR sences in that footprint, becuase we have multiple stacks the graph will be a bit messy:</p> <p></p> <p>Let's check details of our SAR sence stacks and figure out which stack(s) we want to keep: <pre><code>s1.summary()\n</code></pre> This will output the summary of availiable Sentinel-1 scenes that covers the AOI. </p> Output <pre><code>=== ASCENDING ORBITS (14 Stacks) ===\nPath 20 Frame 117 | Count: 10 | 2015-04-05 --&gt; 2016-11-19\nPath 20 Frame 118 | Count: 155 | 2016-12-13 --&gt; 2026-02-12\nPath 20 Frame 119 | Count: 2 | 2015-03-24 --&gt; 2015-12-25\nPath 20 Frame 120 | Count: 12 | 2014-10-31 --&gt; 2016-09-14\nPath 20 Frame 121 | Count: 6 | 2015-04-05 --&gt; 2015-08-27\nPath 20 Frame 122 | Count: 4 | 2016-05-05 --&gt; 2016-11-19\nPath 20 Frame 123 | Count: 150 | 2016-12-13 --&gt; 2026-02-12\nPath 93 Frame 116 | Count: 85 | 2014-11-05 --&gt; 2021-12-16\nPath 93 Frame 117 | Count: 24 | 2015-03-29 --&gt; 2026-02-17\nPath 93 Frame 118 | Count: 5 | 2016-10-07 --&gt; 2017-01-11\nPath 93 Frame 119 | Count: 1 | 2017-02-10 --&gt; 2017-02-10\nPath 93 Frame 120 | Count: 14 | 2015-11-12 --&gt; 2025-07-04\nPath 93 Frame 121 | Count: 85 | 2014-11-05 --&gt; 2021-12-16\nPath 93 Frame 122 | Count: 21 | 2025-05-05 --&gt; 2026-02-17\n\n=== DESCENDING ORBITS (4 Stacks) ===\nPath 100 Frame 464 | Count: 118 | 2015-11-24 --&gt; 2026-02-11\nPath 100 Frame 465 | Count: 20 | 2014-11-29 --&gt; 2017-01-05\nPath 100 Frame 466 | Count: 161 | 2017-02-22 --&gt; 2022-07-02\nPath 100 Frame 469 | Count: 118 | 2015-11-24 --&gt; 2026-02-11\n</code></pre> <p>The program identified 18 potential stacks (14 ascending, 4 descending). We can narrowed the dataset to the descending track Path 100, Frame 466 in year 2020 by:</p> <pre><code>filter_results = s1.filter(path_frame=(100,466), start='2020-01-01', end='2020-12-31')\n</code></pre> <p>Check back the footprint and summary: <pre><code>s1.footprint()\ns1.summary()\n</code></pre> would return: </p> <p> <pre><code>=== DESCENDING ORBITS (1 Stacks) ===\nPath 100 Frame 466 | Count: 30 | 2020-01-02 --&gt; 2020-12-27\n</code></pre></p> <p>Use <code>download</code> to download searched SLC data <pre><code>s1.download()\n</code></pre></p> <p>Use <code>reset</code> to restore original search results.  <pre><code>s1.reset()\n</code></pre></p>"},{"location":"quickstart/running/#interferogram","title":"Interferogram","text":"<p>After locating SAR scene stack(s), the next step is to generate unwrapped interferograms in preparation for the time-series analysis. InSARScript currently support:</p> <ul> <li>HyP3: Use the HyP3 platform  provided by ASF to run the interferometric processing in the cloud and download the resulting interferograms.</li> </ul>"},{"location":"quickstart/running/#hyp3","title":"Hyp3","text":"<p>HyP3 is an online processing platform provided by ASF. InSARSciprt has wrapped hyp3_sdk as a <code>Processor</code>:</p> <p>Hyp3 InSAR Processor takes a pair of <code>reference_granule_id</code> and a <code>secondary_granule_id</code> to generate an interferogram. To automate the pair selection process: </p> <p><pre><code>from insarscript.utils import select_pairs, plot_pair_network\npair_stacks, B = select_pairs(filter_results, max_degree=5) # We set the maximum connections to 5 to limit interferograms\nfig = plot_pair_network(pair_stacks, B)\nfig.show()\n</code></pre> If the network is healthy without any disconnections, you are ready to submit your pairs:  </p> <p>To submit your pairs to Hyp3 server for online interferogram processing:</p> <p><pre><code>from insarscript import Processor\nfor (path, frame), pairs in pair_stacks.items():   \n    processor = Processor.create('Hyp3_InSAR', pairs=pairs, workdir=f'your/directory/p{path}_f{frame}')\n    batch = processor.submit()\n    processor.save()\n</code></pre> This process will generate <code>hyp3_jobs.json</code> under your work directory, which contains the jobID submitted to the hyp3 server, the processing will take roughly 30 mins for 100 interferograms depends on the ASF server load</p> <p>The job script will looks like: </p> Hyp3_jobs <pre><code>{\n\"job_ids\": {\n    \"Your_ASF_UserName\": [\n    \"28a7f5e7-d8d4-4958-854f-a7e625a0a09e\",\n    \"61d48bb8-f86d-4b2a-b933-2709b653b86b\",\n    \"c10cc297-2008-491d-a76c-c38958830d87\",\n    \"68b2be08-2ad7-4cad-9bbb-3d33e8468300\",\n    \"be57c44f-9514-4b92-8e7b-9e1c9c0d9d08\"\n    ...\n    ]\n},\n\"out_dir\": \"/Your/Project/Save/Path\"\n}\n</code></pre> <p>To check the job processing status: </p> <pre><code>processor_reload = Processor.create('Hyp3_InSAR', saved_job_path='your/directory/p*_f*/hyp3_jobs.json')\nbatchs = processor_reload.refresh()\n</code></pre> Output <pre><code>User: jldz9asf (65 jobs)\n\n    JOB NAME                            JOB ID                                 STATUS\n- ifg_20201016T133502_20201109T133501 961b4d1c-df15-4272-843f-390c98f14f50 | SUCCEEDED\n- ifg_20200829T133500_20200910T133501 a449ebf8-1dbc-4a41-a1ae-a6d30deb1fd2 | SUCCEEDED\n- ifg_20200126T133452_20200207T133452 dfef59ce-9e0f-4987-a1fb-84626d58e5a5 | SUCCEEDED\n- ifg_20200606T133455_20200630T133456 1864b4a9-0ae7-4c8f-94e0-eebc334b7bc3 | SUCCEEDED\n- ifg_20201203T133501_20201215T133500 f1f4e7ff-0ed6-46e7-82c2-7e6ba873cdc7 | SUCCEEDED\n- ifg_20200314T133452_20200407T133452 fcd2d9fe-ed0b-4f0c-93dd-d20810b3e15d | SUCCEEDED\n- ifg_20201028T133501_20201121T133501 af6c695b-0cdb-475d-bda1-828723f4dd0b | SUCCEEDED\n- ifg_20200102T133453_20200114T133453 93200c49-6979-482f-8701-8fe6d3f29c08 | SUCCEEDED\n- ifg_20201203T133501_20201227T133500 b749ccdf-1c11-4b0d-8d9a-a962241c624e | SUCCEEDED\n- ifg_20200805T133459_20200817T133459 dbc9ff2f-6d97-4981-b074-25f57838c4aa | SUCCEEDED\n- ifg_20200724T133458_20200817T133459 71f2e751-3302-4504-aab9-bbcec4b9c6eb | SUCCEEDED\n- ifg_20200630T133456_20200712T133457 c10a276e-c575-4162-b00e-7515ff19345c | SUCCEEDED\n- ifg_20200501T133453_20200513T133454 d20fadc7-7d28-4067-ab55-f87d9a4d5063 | SUCCEEDED\n- ifg_20200326T133452_20200407T133452 52985d6c-aae1-4d0e-8c49-8c6aca17dea6 | SUCCEEDED\n- ifg_20200219T133452_20200314T133452 aa233b3b-5476-43c5-8af0-a1ccf0e9233f | SUCCEEDED\n- ifg_20200407T133452_20200419T133453 3e476e94-e2b4-4edc-97bb-0ae07ec13e12 | SUCCEEDED\n- ifg_20200630T133456_20200724T133458 7f6a56f1-b9da-4564-bc95-e5baa1352a22 | SUCCEEDED\n- ifg_20200326T133452_20200501T133453 b5e9aa32-377d-4a2a-8ca1-4035bee888f4 | SUCCEEDED\n- ifg_20201109T133501_20201121T133501 63593204-d6d6-4196-9a30-57b3569e75d1 | SUCCEEDED\n- ifg_20201016T133502_20201028T133501 96d75b89-7198-4436-a3b9-f68e5683e6bc | SUCCEEDED\n- ifg_20200102T133453_20200207T133452 a66bf72a-edf7-47bf-96e4-3bf6ccad874f | SUCCEEDED\n- ifg_20200724T133458_20200805T133459 34cae102-aba2-4d4e-8191-f0f119e53a8a | SUCCEEDED\n- ifg_20201004T133501_20201028T133501 ef81d0e8-45e6-4708-ab59-7658f0488200 | SUCCEEDED\n- ifg_20200219T133452_20200302T133452 5a755364-2e4e-421c-b3c1-6ae46f42b1ac | SUCCEEDED\n- ifg_20200102T133453_20200126T133452 ab2b13eb-0bc3-4ffa-bebb-c7d54cc7b49e | SUCCEEDED\n- ifg_20201109T133501_20201215T133500 1d9cff3f-298f-4ec5-85cc-cbd95df2085a | SUCCEEDED\n- ifg_20201028T133501_20201109T133501 d15552b5-420b-4fd8-99db-b0ac22bb939d | SUCCEEDED\n- ifg_20201109T133501_20201203T133501 2e812368-aea3-4d90-b949-ff7292d548ff | SUCCEEDED\n- ifg_20200910T133501_20201004T133501 a1e25a7a-6dbd-4d75-8672-f4dba2cf973c | SUCCEEDED\n- ifg_20201004T133501_20201016T133502 41085b9b-eeee-4f69-9b1c-834789aaf09b | SUCCEEDED\n- ifg_20200922T133501_20201004T133501 e312e00c-7396-4ab4-a7fe-eaf01cf398f2 | SUCCEEDED\n- ifg_20200817T133459_20200829T133500 7e40a7a8-128a-4b22-a378-7b7099227845 | SUCCEEDED\n- ifg_20200114T133453_20200126T133452 17720d01-9dc7-4ae8-b2b4-68b1aa2cfb5e | SUCCEEDED\n- ifg_20200207T133452_20200219T133452 1760b870-9851-47d5-9d7e-eb4a6c2e3c17 | SUCCEEDED\n- ifg_20200606T133455_20200712T133457 1ed432ff-ece0-451f-8b07-3e14dac2dbf3 | SUCCEEDED\n- ifg_20201121T133501_20201203T133501 d522ba90-2a54-49df-a5ec-d4c47e74854e | SUCCEEDED\n- ifg_20200314T133452_20200419T133453 f0d74aaf-2d0e-45c5-a598-3f59128fd3a4 | SUCCEEDED\n- ifg_20200513T133454_20200606T133455 65ca5370-703f-4382-8358-58602852f4b6 | SUCCEEDED\n- ifg_20200910T133501_20200922T133501 8e21d9a1-7690-4fd5-9e42-b7732556e64f | SUCCEEDED\n- ifg_20200302T133452_20200314T133452 913f582b-7050-4140-912c-1bcd677ed4d0 | SUCCEEDED\n- ifg_20201121T133501_20201215T133500 1b46ebd8-f934-436f-b234-6151bb419f9b | SUCCEEDED\n- ifg_20200407T133452_20200501T133453 86dd0cfd-973f-4453-bd04-a880d14aa3e8 | SUCCEEDED\n- ifg_20200817T133459_20200910T133501 fde1155a-15d9-484d-aead-3594baba5a5b | SUCCEEDED\n- ifg_20200114T133453_20200207T133452 27c88970-ad96-4855-9fdc-55ce865df692 | SUCCEEDED\n- ifg_20200513T133454_20200525T133455 5790c8ab-2186-4ac9-96d6-2015dd36c158 | SUCCEEDED\n- ifg_20201121T133501_20201227T133500 4193ed07-a701-4ebf-9d54-02d635b4fb52 | SUCCEEDED\n- ifg_20200712T133457_20200724T133458 2585422e-9a61-4214-a39b-6af633170074 | SUCCEEDED\n- ifg_20200207T133452_20200302T133452 a8a3e128-4e32-4216-ab6e-5df810fe7b8f | SUCCEEDED\n- ifg_20200419T133453_20200513T133454 f4683c12-2d35-463c-a4a7-d9d19dccd10f | SUCCEEDED\n- ifg_20200805T133459_20200829T133500 2e104b8e-f58c-4a7f-8762-bc5e19c6a588 | SUCCEEDED\n- ifg_20200302T133452_20200326T133452 5909b68d-e171-439a-9337-f5ea8b05bb32 | SUCCEEDED\n- ifg_20200501T133453_20200525T133455 42ff579b-630b-493b-9749-20d89d7b9864 | SUCCEEDED\n- ifg_20200712T133457_20200805T133459 dc8d82b9-930b-4e43-8d4e-91ceb952b5ab | SUCCEEDED\n- ifg_20200525T133455_20200606T133455 863a6c29-6909-4a72-86d1-3da183f6e97a | SUCCEEDED\n- ifg_20200525T133455_20200630T133456 70b7a263-3888-4780-88fc-2f3cdd176bbb | SUCCEEDED\n- ifg_20200326T133452_20200419T133453 46ca5dc1-ec65-42b8-ad6c-63c860be7f61 | SUCCEEDED\n- ifg_20200922T133501_20201016T133502 8ca82ac5-3a8a-4f8d-85b1-49bbe1cbbd1d | SUCCEEDED\n- ifg_20200922T133501_20201028T133501 1e6bc45c-4ce2-4fb9-9179-2818196b801b | SUCCEEDED\n- ifg_20201215T133500_20201227T133500 b5aae0e8-99de-4ec3-ad85-40e45a5e1b69 | SUCCEEDED\n- ifg_20200114T133453_20200219T133452 5b18c065-d74b-4ee4-87a8-acce835252b1 | SUCCEEDED\n- ifg_20200314T133452_20200326T133452 6c17f1e6-0b43-45fe-83cd-297d4cb64f2a | SUCCEEDED\n- ifg_20200829T133500_20200922T133501 2259ce4a-d21f-4dc0-8da6-38d991086111 | SUCCEEDED\n- ifg_20200302T133452_20200407T133452 5bf4a092-ad5a-48e2-9095-855b3604be9b | SUCCEEDED\n- ifg_20200419T133453_20200501T133453 37c7bf39-4634-484f-a177-ae8bf314d1ba | SUCCEEDED\n- ifg_20200126T133452_20200219T133452 7c5d6fb2-4bf4-44c9-adbd-777ed3e13dc3 | SUCCEEDED\n</code></pre> <p>Seems all interferograms are ready! (I have failed jobs?)</p> <p>Processed interferograms can be downloaded by: </p> <p><pre><code>processor_reload.download()\n</code></pre> Once all downloads are complete, we are ready to move to time-series analysis!</p>"},{"location":"quickstart/running/#time-series-analysis","title":"Time-series Analysis","text":"<p>After generated all unwrapped interferograms, time-series analysis is recommended for long term deformation monitoring. InSARScript currently support:</p> <ul> <li>Mintpy: an open-source Python package for InSAR time-series analysis.</li> </ul>"},{"location":"quickstart/running/#mintpy","title":"Mintpy","text":"<p>InSARSciprt has wrapped Mintpy's <code>SmallbaselineApp</code> as an analyzer, to connect Mintpy with Hyp3 product: </p> <p><pre><code>from insarscript import Analyzer\nworkdir = 'your/directory/p*_f*'\nhyp3_sbas = Analyzer.create('Hyp3_SBAS', workdir=workdir)\nhyp3_sbas.prep_data()\nhyp3_sbas.run()\n</code></pre> The SBAS analysis will be up and running.</p>"},{"location":"quickstart/running/#post-processing","title":"Post-Processing","text":"<p>Normally Analyzer like Mintpy will handle postprocessing automatically.</p>"},{"location":"quickstart/utilities/","title":"Utilities","text":"<p>Utilities are sets of tools designed to support and streamline InSAR processing workflows.</p>"},{"location":"quickstart/utilities/#select-pairs","title":"Select Pairs","text":"<p>Select interferogram pairs from ASF search results based on temporal and     perpendicular baseline criteria.</p> <pre><code>from insarscript import Downloader\nfrom insarscript.utils import select_pairs \ns1 = Downloader.create('S1_SLC', \n                    intersectsWith=[-113.05, 37.74, -112.68, 38.00],\n                    start='2020-01-01', \n                    end='2020-12-31',  \n                    relativeOrbit=100, \n                    frame=466, \n                    workdir='path/to/dir')\nresults = dl.search()\n\npairs, baselines = select_pairs(search_results=results)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>search_results</code> <code>list[ASFProduct] | dict[tuple[int, int], list[ASFProduct]]</code> <p>Either a flat list (single stack) or a dictionary keyed by (path, frame).</p> required <code>dt_targets</code> <code>list[float]</code> <p>Preferred temporal spacings in days. A candidate pair passes if  |dt - target| &lt;= dt_tol for at least one target.</p> <code>(6, 12, 24, 36, 48, 72, 96)</code> <code>dt_tol</code> <code>float</code> <p>Tolerance in days added to each entry in dt_targets.</p> <code>3</code> <code>dt_max</code> <code>float</code> <p>Maximum temporal baseline in days.</p> <code>120</code> <code>pb_max</code> <code>float</code> <p>Maximum perpendicular baseline in meters.</p> <code>150.0</code> <code>min_degree</code> <code>int</code> <p>Minimum interferogram connections per scene. Enforced when force_connect is True.</p> <code>3</code> <code>max_degree</code> <code>int</code> <p>Maximum interferogram connections per scene.</p> <code>999</code> <code>force_connect</code> <code>bool</code> <p>If a scene falls below min_degree after primary selection, add its nearest-time  neighbors that satisfy pb_max and dt_max. May introduce lower-quality pairs; a warning is logged.</p> <code>True</code> <code>max_workers</code> <code>int</code> <p>Number of threads for API fallback. Has no effect if all products have local baseline  data (common for Sentinel-1 and ALOS). Set to 1 to disable threading (useful for debugging).</p> <code>8</code> Source code in <code>src/insarscript/utils/tool.py</code> <pre><code>def select_pairs(\n    search_results: Union[dict[tuple[int, int], list[ASFProduct]], list[ASFProduct]],\n    dt_targets: tuple[int, ...] = (6, 12, 24, 36, 48, 72, 96),\n    dt_tol: int = 3,\n    dt_max: int = 120,\n    pb_max: float = 150.0,\n    min_degree: int = 3,\n    max_degree: int = 999,\n    force_connect: bool = True,\n    max_workers: int = 8\n) -&gt; Union[PairGroup, list[Pair]]:\n\n    \"\"\"\n    Select interferogram pairs based on temporal and perpendicular baseline.\n\n    This function selects interferogram pairs according to temporal spacing \n    and perpendicular baseline constraints, optionally enforcing connectivity \n    rules per scene.\n\n    Supported sensors:\n    - Sentinel-1 (CALCULATED)  : stateVectors + ascendingNodeTime \u2192 local\n    - ALOS / ERS / RADARSAT (PRE_CALCULATED) : insarBaseline scalar \u2192 local\n    - Any product missing data : ref.stack() API call \u2192 fallback\n\n    Args:\n        search_results (list[ASFProduct] | dict[tuple[int,int], list[ASFProduct]]):\n            Either a flat list (single stack) or a dictionary keyed by (path, frame).\n        dt_targets (list[float], optional):\n            Preferred temporal spacings in days. A candidate pair passes if \n            |dt - target| &lt;= dt_tol for at least one target.\n        dt_tol (float, optional):\n            Tolerance in days added to each entry in dt_targets.\n        dt_max (float, optional):\n            Maximum temporal baseline in days.\n        pb_max (float, optional):\n            Maximum perpendicular baseline in meters.\n        min_degree (int, optional):\n            Minimum interferogram connections per scene. Enforced when force_connect is True.\n        max_degree (int, optional):\n            Maximum interferogram connections per scene.\n        force_connect (bool, optional):\n            If a scene falls below min_degree after primary selection, add its nearest-time \n            neighbors that satisfy pb_max and dt_max. May introduce lower-quality pairs; a warning is logged.\n        max_workers (int, optional):\n            Number of threads for API fallback. Has no effect if all products have local baseline \n            data (common for Sentinel-1 and ALOS). Set to 1 to disable threading (useful for debugging).\n\n    Returns:\n        list[Pair] | dict[tuple[int,int], list[Pair]]:\n            A flat list of Pair tuples (earlier_scene_name, later_scene_name) sorted by acquisition time, \n            if search_results was a list. Otherwise, a dictionary keyed by (path, frame) with lists of Pair tuples.\n    \"\"\"\n\n    # \u2500\u2500 normalise input \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    input_is_list = isinstance(search_results, list)\n    if input_is_list:\n        working_dict: dict[tuple[int, int], list[ASFProduct]] = {\n            (0, 0): search_results   # type: ignore[arg-type]\n        }\n    elif isinstance(search_results, dict):\n        working_dict = search_results\n    else:\n        raise TypeError(\n            f\"search_results must be a list or dict of ASFProducts, \"\n            f\"got {type(search_results)}\"\n        )\n\n    # \u2500\u2500 primary filter helpers (defined once, closed over threshold args) \u2500\n    def _near_target(dt: float) -&gt; bool:\n        return any(abs(dt - t) &lt;= dt_tol for t in dt_targets)\n\n    def _passes_primary(dt: float, bp: float) -&gt; bool:\n        return _near_target(dt) and dt &lt;= dt_max and bp &lt;= pb_max\n\n    pairs_group: PairGroup = defaultdict(list)\n\n    # \u2500\u2500 process each (path, frame) key \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    for key, search_result in working_dict.items():\n        if not input_is_list:\n            logger.info(\n                \"%sSearching pairs for path %d frame %d \u2026\",\n                Fore.GREEN, key[0], key[1],\n            )\n\n        # Sort by acquisition time so `names` is chronologically ordered\n        prods = sorted(search_result, key=lambda p: p.properties[\"startTime\"])\n\n        if not prods:\n            logger.warning(\"No products for key %s \u2014 skipping.\", key)\n            continue\n\n        # Pre-parse acquisition datetimes to Unix timestamps (done once;\n        # reused in sort keys, dt calculations, and pair ordering)\n        id_time_raw: dict[SceneID, str] = {\n            p.properties[\"sceneName\"]: p.properties[\"startTime\"] for p in prods\n        }\n        id_time_dt: dict[SceneID, DateFloat] = {\n            sid: isoparse(t).timestamp() for sid, t in id_time_raw.items()\n        }\n        ids: set[SceneID] = set(id_time_raw)\n        names: list[SceneID] = [p.properties[\"sceneName\"] for p in prods]\n\n        # \u2500\u2500 1. Build pairwise baseline table \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        B = _build_baseline_table(prods, ids, id_time_dt, max_workers=max_workers)\n\n        # \u2500\u2500 2. Primary pair selection \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        pairs: set[Pair] = {\n            e for e, (dt, bp) in B.items() if _passes_primary(dt, bp)\n        }\n        logger.info(\n            \"Key %s \u2014 primary selection: %d / %d candidate pairs.\",\n            key, len(pairs), len(B),\n        )\n\n        # \u2500\u2500 3. Connectivity enforcement \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n        pairs = _enforce_connectivity(\n            pairs,\n            B,\n            names,\n            id_time_dt,\n            min_degree=min_degree,\n            max_degree=max_degree,\n            pb_max=pb_max,\n            dt_max=float(dt_max),\n            force_connect=force_connect\n        )\n\n        pairs_group[key] = sorted(pairs)\n        logger.info(\n            \"Key %s \u2014 final pair count: %d.\", key, len(pairs_group[key])\n        )\n    pairs = pairs_group[(0, 0)] if input_is_list else pairs_group\n\n    return pairs, B\n</code></pre>"},{"location":"quickstart/utilities/#plot-pair-network","title":"Plot Pair Network","text":"<p>Plot selected interferogram pairs SBAS network from select_pairs based on temporal and     perpendicular baseline criteria. </p> <pre><code>from insarscript.utils import plot_pair_network\n\nfig = plot_pair_network(pairs=pairs, baselines=baselines)\n</code></pre> <p>Example: </p> <p></p> <p>Parameters:</p> Name Type Description Default <code>pairs</code> <code>list[Pair] | PairGroup</code> <p>A flat list of pairs or a dictionary keyed by (path, frame) with lists of pairs. Each pair is a tuple <code>(earlier_scene, later_scene)</code>.</p> required <code>baselines</code> <code>BaselineTable</code> <p>Table or mapping containing temporal and perpendicular baseline information for each interferogram pair.</p> required <code>title</code> <code>str</code> <p>Main title of the network plot. Defaults to \"Interferogram Network\".</p> <code>'Interferogram Network'</code> <code>figsize</code> <code>tuple[int, int]</code> <p>Figure size (width, height) in inches. Defaults to (18, 7).</p> <code>(18, 7)</code> <code>save_path</code> <code>str | Path | None</code> <p>Path to save the generated figure. If None, figure is not saved. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any scene name in <code>pairs</code> is not a string.</p> <code>ValueError</code> <p>If a scene name cannot be parsed into a valid date.</p> Source code in <code>src/insarscript/utils/tool.py</code> <pre><code>def plot_pair_network(\n    pairs: list[Pair] | PairGroup,\n    baselines: BaselineTable,                           \n    title: str = \"Interferogram Network\",\n    figsize: tuple[int, int] = (18, 7),\n    save_path: str |Path| None = None,\n) -&gt; plt.Figure:\n\n    \"\"\"\n    Plot an interferogram network along with per-scene connection statistics.\n\n    This function visualizes the relationships between SAR acquisitions in\n    terms of temporal and perpendicular baselines. The network graph is\n    shown on the left, while a horizontal bar chart summarizes the number\n    of connections per scene on the right.\n\n    The layout is as follows:\n        - Left  : Network graph (x-axis = days since first acquisition,\n                  y-axis = perpendicular baseline [m])\n        - Right : Horizontal bar chart showing the number of connections per SAR scene\n\n    Args:\n        pairs (list[Pair] | PairGroup):\n            A flat list of pairs or a dictionary keyed by (path, frame)\n            with lists of pairs. Each pair is a tuple `(earlier_scene, later_scene)`.\n        baselines (BaselineTable):\n            Table or mapping containing temporal and perpendicular baseline\n            information for each interferogram pair.\n        title (str, optional):\n            Main title of the network plot. Defaults to \"Interferogram Network\".\n        figsize (tuple[int, int], optional):\n            Figure size (width, height) in inches. Defaults to (18, 7).\n        save_path (str | Path | None, optional):\n            Path to save the generated figure. If None, figure is not saved.\n            Defaults to None.\n\n    Returns:\n        matplotlib.figure.Figure:\n            The created matplotlib figure containing the network and\n            per-scene connection histogram.\n\n    Raises:\n        TypeError:\n            If any scene name in `pairs` is not a string.\n        ValueError:\n            If a scene name cannot be parsed into a valid date.\n\n    Notes:\n        - Node positions: x = days since first acquisition, y = perpendicular baseline.\n        - Node color represents the node degree (number of connections).\n        - Edge color and width represent temporal baseline.\n        - Scenes with fewer than 2 connections are highlighted in red in the histogram.\n        - Legends show node degree, temporal baseline, and path/frame grouping.\n        - The top axis of the network plot shows real acquisition dates for reference.\n    \"\"\"\n\n    # \u2500\u2500 0. Normalise input \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    if save_path is not None:\n        save_path = Path(save_path).expanduser()\n\n    if isinstance(pairs, dict):\n        flat_pairs: list[Pair] = []\n        group_labels: list[str] = []\n        for (path, frame), pair_list in pairs.items():\n            flat_pairs.extend(pair_list)\n            group_labels.append(f\"P{path}/F{frame}: {len(pair_list)} pairs\")\n        subtitle = \" | \".join(group_labels)\n    else:\n        flat_pairs = pairs\n        subtitle = f\"{len(flat_pairs)} pairs\"\n\n    # \u2500\u2500 1. Parse dates \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    scenes: set[SceneID] = set()\n    for a, b in flat_pairs:\n        scenes.update([a, b])\n\n    def _parse_date(scene_name: str) -&gt; datetime:\n        if not isinstance(scene_name, str):\n            raise TypeError(\n                f\"Expected str, got {type(scene_name).__name__}: {scene_name!r}.\"\n            )\n        m = re.search(r\"(\\d{8})\", scene_name)\n        if m:\n            return datetime.strptime(m.group(1), \"%Y%m%d\")\n        m = re.search(r\"(\\d{4}-\\d{2}-\\d{2})\", scene_name)\n        if m:\n            return datetime.strptime(m.group(1), \"%Y-%m-%d\")\n        raise ValueError(f\"Cannot parse date from scene name: {scene_name}\")\n\n    id_time: dict[SceneID, datetime] = {s: _parse_date(s) for s in scenes}\n    t0      = min(id_time.values())\n    id_days: dict[SceneID, float] = {\n        s: (id_time[s] - t0).total_seconds() / 86_400.0 for s in scenes\n    }\n\n    # \u2500\u2500 2. Build graph \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    G = nx.Graph()\n    G.add_nodes_from(scenes)\n\n    if isinstance(pairs, dict):\n        for (path, frame), pair_list in pairs.items():\n            for a, b in pair_list:\n                dt, bp = baselines.get((a, b), (_MISSING, _MISSING))\n                G.add_edge(a, b, dt=dt, bp=bp, path=path, frame=frame)\n    else:\n        for a, b in flat_pairs:\n            dt, bp = baselines.get((a, b), (_MISSING, _MISSING))\n            G.add_edge(a, b, dt=dt, bp=bp, path=0, frame=0)\n\n    # \u2500\u2500 3. Node positions (x=days, y=bperp) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    # Assign each scene a bperp position by averaging bperp of all its pairs.\n    # baselines stores absolute bperp; we recover a relative position by anchoring\n    # the earliest scene at y=0 and walking forward in time.\n    bperp_accum: dict[SceneID, list[float]] = defaultdict(list)\n    for (a, b), (dt, bp) in baselines.items():\n        if bp &gt;= _MISSING:\n            continue\n        bperp_accum[a].append(-bp / 2.0)\n        bperp_accum[b].append(+bp / 2.0)\n\n    bperp_pos: dict[SceneID, float] = {\n        s: float(np.mean(v)) if v else 0.0\n        for s, v in bperp_accum.items()\n    }\n    # anchor earliest scene to y=0\n    sorted_by_time = sorted(scenes, key=lambda s: id_days[s])\n    offset = bperp_pos.get(sorted_by_time[0], 0.0)\n    bperp_pos = {s: bperp_pos.get(s, 0.0) - offset for s in scenes}\n\n    pos: dict[SceneID, tuple[float, float]] = {\n        s: (id_days[s], bperp_pos[s]) for s in scenes\n    }\n\n    # \u2500\u2500 4. Visual attributes \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    degrees      = dict(G.degree())\n    max_deg      = max(degrees.values(), default=1)\n    node_colours = [plt.cm.RdYlGn(degrees[n] / max_deg) for n in G.nodes()]\n\n    edge_dts     = [G[a][b][\"dt\"] for a, b in G.edges()]\n    max_dt       = max((d for d in edge_dts if d &lt; _MISSING), default=1.0)\n    edge_colours = [plt.cm.RdYlGn_r(min(dt, max_dt) / max_dt) for dt in edge_dts]\n    edge_widths  = [0.5 + 2.5 * (1.0 - min(dt, max_dt) / max_dt) for dt in edge_dts]\n\n    if isinstance(pairs, dict):\n        group_keys  = list(pairs.keys())\n        linestyles  = [\"-\", \"--\", \"-.\", \":\"] * (len(group_keys) // 4 + 1)\n        key_style   = {k: linestyles[i] for i, k in enumerate(group_keys)}\n        edge_styles = [\n            key_style[(G[a][b][\"path\"], G[a][b][\"frame\"])] for a, b in G.edges()\n        ]\n    else:\n        edge_styles = [\"-\"] * len(G.edges())\n\n    # \u2500\u2500 5. Figure layout \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    fig = plt.figure(figsize=figsize)\n    gs  = fig.add_gridspec(1, 2, width_ratios=[3, 1], wspace=0.35)\n    ax_net  = fig.add_subplot(gs[0])\n    ax_hist = fig.add_subplot(gs[1])\n\n    # \u2500\u2500 6. Draw network \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    edges_by_style: dict[str, list] = defaultdict(list)\n    for (a, b), style, colour, width in zip(\n        G.edges(), edge_styles, edge_colours, edge_widths\n    ):\n        edges_by_style[style].append((a, b, colour, width))\n\n    for style, edge_data in edges_by_style.items():\n        nx.draw_networkx_edges(\n            G, pos, ax=ax_net,\n            edgelist=[(a, b) for a, b, _, _ in edge_data],\n            edge_color=[c for _, _, c, _ in edge_data],\n            width=[w for _, _, _, w in edge_data],\n            style=style,\n            alpha=0.7,\n        )\n\n    nx.draw_networkx_nodes(\n        G, pos, ax=ax_net,\n        node_color=node_colours,\n        node_size=80,\n        linewidths=0.5,\n        edgecolors=\"black\",\n    )\n    nx.draw_networkx_labels(\n        G, pos,\n        labels={s: s[-8:] for s in G.nodes()},\n        ax=ax_net,\n        font_size=5,\n    )\n\n    # \u2500\u2500 7. Network axes \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    ax_net.set_xlabel(\"Days since first acquisition\", fontsize=11)\n    ax_net.set_ylabel(\"Perpendicular baseline [m]\", fontsize=11)    # \u2705 real unit\n    ax_net.set_title(\n        f\"{title}\\n{subtitle}\\n\"\n        f\"{len(scenes)} scenes \u00b7 {len(flat_pairs)} pairs \u00b7 \"\n        f\"mean degree {np.mean(list(degrees.values())):.1f}\",\n        fontsize=11,\n    )\n    ax_net.tick_params(left=True, bottom=True, labelleft=True, labelbottom=True)\n    ax_net.set_frame_on(True)\n\n    # real date ticks on top axis\n    x_vals  = [p[0] for p in pos.values()]\n    x_ticks = np.linspace(min(x_vals), max(x_vals), min(8, len(pos)))\n    ax2 = ax_net.twiny()\n    ax2.set_xlim(ax_net.get_xlim())\n    ax2.set_xticks(x_ticks)\n    ax2.set_xticklabels(\n        [\n            (t0 + __import__(\"datetime\").timedelta(days=d)).strftime(\"%Y-%m-%d\")\n            for d in x_ticks\n        ],\n        rotation=30, ha=\"left\", fontsize=7,\n    )\n    ax2.set_xlabel(\"Acquisition date (UTC)\", fontsize=9)\n\n    # \u2500\u2500 8. Per-scene connection histogram \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    # Sort scenes by date so the histogram reads chronologically top\u2192bottom\n    sorted_scene_names = sorted(scenes, key=lambda s: id_days[s])\n    scene_degrees      = [degrees[s] for s in sorted_scene_names]\n    short_names        = [s[-12:] for s in sorted_scene_names]   # trim for readability\n    y_positions        = range(len(sorted_scene_names))\n\n    bar_colours = [plt.cm.RdYlGn(degrees[s] / max_deg) for s in sorted_scene_names]\n\n    bars = ax_hist.barh(\n        y_positions,\n        scene_degrees,\n        color=bar_colours,\n        edgecolor=\"white\",\n        linewidth=0.4,\n        height=0.7,\n    )\n\n    # annotate each bar with connection count\n    for bar, count in zip(bars, scene_degrees):\n        ax_hist.text(\n            bar.get_width() + 0.1,\n            bar.get_y() + bar.get_height() / 2,\n            str(count),\n            va=\"center\", fontsize=7,\n        )\n\n    # vertical line at mean degree\n    mean_deg = np.mean(scene_degrees)\n    ax_hist.axvline(\n        mean_deg, color=\"steelblue\", linestyle=\"--\", linewidth=1.0, alpha=0.8\n    )\n    ax_hist.text(\n        mean_deg + 0.1, len(sorted_scene_names) - 0.5,\n        f\"mean\\n{mean_deg:.1f}\",\n        color=\"steelblue\", fontsize=7, va=\"top\",\n    )\n\n    # mark scenes below min connectivity in red\n    for i, (s, deg) in enumerate(zip(sorted_scene_names, scene_degrees)):\n        if deg &lt; 2:\n            ax_hist.get_children()[i].set_edgecolor(\"red\")\n            ax_hist.get_children()[i].set_linewidth(1.5)\n\n    ax_hist.set_yticks(y_positions)\n    ax_hist.set_yticklabels(short_names, fontsize=6)\n    ax_hist.set_xlabel(\"Number of connections\", fontsize=9)\n    ax_hist.set_title(\"Connections\\nper scene\", fontsize=10)\n    ax_hist.xaxis.set_major_locator(plt.MaxNLocator(integer=True))\n    ax_hist.set_frame_on(True)\n    # match vertical order to network: earliest at top\n    ax_hist.invert_yaxis()\n\n    # \u2500\u2500 9. Legends \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    deg_legend = ax_net.legend(\n        handles=[\n            mpatches.Patch(color=plt.cm.RdYlGn(v / max_deg), label=f\"degree {v}\")\n            for v in sorted(set(degrees.values()))\n        ],\n        title=\"Node degree\", loc=\"upper left\", fontsize=7, title_fontsize=8,\n    )\n    ax_net.add_artist(deg_legend)\n\n    ax_net.legend(\n        handles=[\n            mpatches.Patch(\n                color=plt.cm.RdYlGn_r(v / max_dt), label=f\"{v:.0f} days\"\n            )\n            for v in [0, max_dt * 0.33, max_dt * 0.66, max_dt]\n        ],\n        title=\"Temporal baseline\", loc=\"lower right\", fontsize=7, title_fontsize=8,\n    )\n\n    if isinstance(pairs, dict):\n        ax_net.add_artist(\n            ax_net.legend(\n                handles=[\n                    mpatches.Patch(\n                        linestyle=key_style[k], fill=False,\n                        edgecolor=\"grey\", label=f\"P{k[0]}/F{k[1]}\",\n                    )\n                    for k in group_keys\n                ],\n                title=\"Path / Frame\", loc=\"upper right\", fontsize=7, title_fontsize=8,\n            )\n        )\n\n    if save_path:\n        fig.savefig(save_path.as_posix(), dpi=300, bbox_inches=\"tight\")\n        print(f\"Saved \u2192 {save_path}\")\n\n    return fig\n</code></pre>"},{"location":"quickstart/utilities/#earth-credit-pool","title":"Earth Credit Pool","text":"<p>If user have multiple Earthdata credentials, user may storage it under ~/.credit_pool with format  <pre><code>username1:password1\nusername2:password2\n</code></pre> then read use: <pre><code>from isnarscript.utils import earth_credit_pool\nec_pool = earth_credit_pool()\n</code></pre> You may then pass this into processor for seameless switch across multiple Earthdata credentials</p> <pre><code>from insarscript import Processor\nprocessor= Processor.create('Hyp3_InSAR', earthdata_credentials_pool=ec_pool, ....)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>earthdata_credentials_pool_path</code> <code>Path</code> <p>Path to the Earthdata credentials file. Defaults to <code>~/.credit_pool</code>. The path is expanded and resolved to an absolute path.</p> <code>joinpath('.credit_pool')</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the specified credentials file does not exist.</p> <code>ValueError</code> <p>If any line in the file does not contain a single ':' separating key and value.</p> <code>OSError</code> <p>For any other I/O related errors while reading the file.</p> Source code in <code>src/insarscript/utils/tool.py</code> <pre><code>def earth_credit_pool(earthdata_credentials_pool_path = Path.home().joinpath('.credit_pool')) -&gt; dict:\n    \"\"\"\n    Load Earthdata credentials from a local credit pool file.\n\n    The function reads a simple key-value file where each line contains\n    `username:password` (or `key:value`) pairs, and returns them as a dictionary.\n\n    Args:\n        earthdata_credentials_pool_path (Path, optional):\n            Path to the Earthdata credentials file. Defaults to\n            `~/.credit_pool`. The path is expanded and resolved to an absolute path.\n\n    Returns:\n        dict:\n            Dictionary mapping credential keys to their corresponding values.\n            Example:\n            ```\n            {\n                \"username1\": \"password1\",\n                \"username2\": \"password2\",\n            }\n            ```\n\n    Raises:\n        FileNotFoundError:\n            If the specified credentials file does not exist.\n        ValueError:\n            If any line in the file does not contain a single ':' separating key and value.\n        OSError:\n            For any other I/O related errors while reading the file.\n\n    Notes:\n        - Each line of the file must be formatted as `key:value`.\n        - Leading/trailing whitespace is stripped from both key and value.\n        - Useful for managing multiple Earthdata credentials for automated downloads.\n    \"\"\"\n    earthdata_credentials_pool_path = Path(earthdata_credentials_pool_path).expanduser().resolve()\n    earthdata_credentials_pool = {}\n    with open(earthdata_credentials_pool_path, 'r') as f:\n        for line in f:\n            key, value = line.strip().split(':')\n            earthdata_credentials_pool[key] = value\n    return earthdata_credentials_pool\n</code></pre>"},{"location":"quickstart/utilities/#slurm-job-config","title":"Slurm Job Config","text":"<p>This class encapsulates all parameters needed to generate a SLURM batch script,     including resource allocation, job settings, environment configuration, and     execution commands.</p> <pre><code>from insarscript.utils import Slurmjob_Config\nconfig = SlurmJobConfig(\n            job_name=\"my_analysis\",\n            time=\"02:00:00\",\n            command=\"python analyze.py\"\n        )\nconfig.to_script(\"analysis.slurm\")\n</code></pre> <p>Attributes:</p> Name Type Description <code>job_name</code> <code>str</code> <p>Name of the SLURM job.</p> <code>output_file</code> <code>str</code> <p>Path for standard output. Use %j for job ID.</p> <code>error_file</code> <code>str</code> <p>Path for standard error. Use %j for job ID.</p> <code>time</code> <code>str</code> <p>Maximum wall time in HH:MM:SS format.</p> <code>partition</code> <code>str</code> <p>SLURM partition name to submit to.</p> <code>nodes</code> <code>int</code> <p>Number of nodes to allocate.</p> <code>ntasks</code> <code>int</code> <p>Number of tasks to run.</p> <code>cpus_per_task</code> <code>int</code> <p>CPUs per task.</p> <code>mem</code> <code>str</code> <p>Memory allocation per node (e.g., \"4G\", \"500M\").</p> <code>nodelist</code> <code>Optional[str]</code> <p>Specific nodes to use (e.g., \"node[01-05]\").</p> <code>gpus</code> <code>Optional[str]</code> <p>GPU allocation (e.g., \"1\", \"2\", \"1g\").</p> <code>array</code> <code>Optional[str]</code> <p>Array job specification (e.g., \"0-9\", \"1-100%10\").</p> <code>dependency</code> <code>Optional[str]</code> <p>Job dependency condition (e.g., \"afterok:123456\").</p> <code>mail_user</code> <code>Optional[str]</code> <p>Email address for job notifications.</p> <code>mail_type</code> <code>str</code> <p>When to send email notifications (BEGIN, END, FAIL, ALL).</p> <code>account</code> <code>Optional[str]</code> <p>Account to charge resources to.</p> <code>qos</code> <code>Optional[str]</code> <p>Quality of Service specification.</p> <code>modules</code> <code>List[str]</code> <p>List of environment modules to load.</p> <code>conda_env</code> <code>Optional[str]</code> <p>Name of conda environment to activate.</p> <code>export_env</code> <code>Dict[str, str]</code> <p>Dictionary of environment variables to export.</p> <code>command</code> <code>str</code> <p>Bash command(s) to execute.</p> Source code in <code>src/insarscript/utils/tool.py</code> <pre><code>@dataclass\nclass Slurmjob_Config:\n    \"\"\"Configuration for a SLURM job submission script.\n\n    This class encapsulates all parameters needed to generate a SLURM batch script,\n    including resource allocation, job settings, environment configuration, and\n    execution commands.\n\n    Attributes:\n        job_name: Name of the SLURM job.\n        output_file: Path for standard output. Use %j for job ID.\n        error_file: Path for standard error. Use %j for job ID.\n        time: Maximum wall time in HH:MM:SS format.\n        partition: SLURM partition name to submit to.\n        nodes: Number of nodes to allocate.\n        ntasks: Number of tasks to run.\n        cpus_per_task: CPUs per task.\n        mem: Memory allocation per node (e.g., \"4G\", \"500M\").\n        nodelist: Specific nodes to use (e.g., \"node[01-05]\").\n        gpus: GPU allocation (e.g., \"1\", \"2\", \"1g\").\n        array: Array job specification (e.g., \"0-9\", \"1-100%10\").\n        dependency: Job dependency condition (e.g., \"afterok:123456\").\n        mail_user: Email address for job notifications.\n        mail_type: When to send email notifications (BEGIN, END, FAIL, ALL).\n        account: Account to charge resources to.\n        qos: Quality of Service specification.\n        modules: List of environment modules to load.\n        conda_env: Name of conda environment to activate.\n        export_env: Dictionary of environment variables to export.\n        command: Bash command(s) to execute.\n\n    Examples:\n        Basic job configuration:\n\n        &gt;&gt;&gt; config = SlurmJobConfig(\n        ...     job_name=\"my_analysis\",\n        ...     time=\"02:00:00\",\n        ...     command=\"python analyze.py\"\n        ... )\n        &gt;&gt;&gt; config.to_script(\"analysis.slurm\")\n        PosixPath('analysis.slurm')\n\n        GPU job with conda environment:\n\n        &gt;&gt;&gt; config = SlurmJobConfig(\n        ...     job_name=\"training\",\n        ...     time=\"12:00:00\",\n        ...     mem=\"32G\",\n        ...     gpus=\"2\",\n        ...     conda_env=\"pytorch\",\n        ...     modules=[\"cuda/11.8\"],\n        ...     command=\"python train.py --epochs 100\"\n        ... )\n        &gt;&gt;&gt; config.to_script(\"train.slurm\")\n        PosixPath('train.slurm')\n\n        Array job with environment variables:\n\n        &gt;&gt;&gt; config = SlurmJobConfig(\n        ...     job_name=\"param_sweep\",\n        ...     array=\"0-99\",\n        ...     export_env={\"PARAM_ID\": \"$SLURM_ARRAY_TASK_ID\"},\n        ...     command=\"python run_experiment.py $PARAM_ID\"\n        ... )\n        &gt;&gt;&gt; config.to_script()\n        PosixPath('job.slurm')\n    \"\"\"\n    job_name: str = \"my_job\"\n    output_file: str = \"job_%j.out\"\n    error_file: str = \"job_%j.err\"\n    time: str = \"04:00:00\"\n    partition: str = \"all\"\n    nodes: int = 1\n    ntasks: int = 1\n    cpus_per_task: int = 1\n    mem: str = \"4G\"\n\n    # Optional parameters\n    nodelist: Optional[str] = None\n    gpus: Optional[str] = None\n    array: Optional[str] = None\n    dependency: Optional[str] = None\n    mail_user: Optional[str] = None\n    mail_type: str = \"ALL\"\n    account: Optional[str] = None\n    qos: Optional[str] = None\n\n    # Environment\n    modules: List[str] = field(default_factory=list)\n    conda_env: Optional[str] = None\n    export_env: Dict[str, str] = field(default_factory=dict)\n\n    # Execution\n    command: str = \"echo Hello SLURM!\"\n\n    def to_script(self, filename: str = \"job.slurm\") -&gt; Path:\n        \"\"\"Generate the SLURM script file.\"\"\"\n        lines = [\"#!/bin/bash\"]\n\n        # Required directives\n        lines.extend([\n            f\"#SBATCH --job-name={self.job_name}\",\n            f\"#SBATCH --output={self.output_file}\",\n            f\"#SBATCH --error={self.error_file}\",\n            f\"#SBATCH --time={self.time}\",\n            f\"#SBATCH --partition={self.partition}\",\n            f\"#SBATCH --nodes={self.nodes}\",\n            f\"#SBATCH --ntasks={self.ntasks}\",\n            f\"#SBATCH --cpus-per-task={self.cpus_per_task}\",\n            f\"#SBATCH --mem={self.mem}\",\n        ])\n\n        # Optional directives\n        if self.gpus:\n            lines.append(f\"#SBATCH --gres=gpu:{self.gpus}\")\n        if self.array:\n            lines.append(f\"#SBATCH --array={self.array}\")\n        if self.dependency:\n            lines.append(f\"#SBATCH --dependency={self.dependency}\")\n        if self.mail_user:\n            lines.append(f\"#SBATCH --mail-user={self.mail_user}\")\n            lines.append(f\"#SBATCH --mail-type={self.mail_type}\")\n        if self.account:\n            lines.append(f\"#SBATCH --account={self.account}\")\n        if self.qos:\n            lines.append(f\"#SBATCH --qos={self.qos}\")\n        if self.nodelist:\n            lines.append(f\"#SBATCH --nodelist={self.nodelist}\")\n\n        lines.append(\"\")\n\n        # Environment setup\n        lines.extend([f\"module load {mod}\" for mod in self.modules])\n        if self.conda_env:\n            lines.append(f\"source activate {self.conda_env}\")\n        lines.extend([f\"export {k}={v}\" for k, v in self.export_env.items()])\n\n        lines.append(\"\")\n\n        # Execution\n        lines.extend([\n            'echo \"Starting job on $(date)\"',\n            self.command,\n            'echo \"Job finished on $(date)\"'\n        ])\n\n        filepath = Path(filename).expanduser().resolve()\n        filepath.write_text(\"\\n\".join(lines))\n\n        return filepath\n</code></pre>"}]}